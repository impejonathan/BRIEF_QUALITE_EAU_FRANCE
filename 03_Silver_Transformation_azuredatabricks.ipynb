{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f79545b-c881-46d8-a65f-843d1dbadb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Configuration initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd4090c-5341-4d92-96c4-cc6ff2b7ab36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import year, month, to_date, col\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß CONFIGURATION INITIALE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Variables d'environnement\n",
    "storage_account_name = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "\n",
    "if storage_account_name and storage_account_key:\n",
    "    # Configuration Spark\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "        storage_account_key\n",
    "    )\n",
    "    print(\"‚úÖ Configuration Spark effectu√©e\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Variables d'environnement non configur√©es\")\n",
    "\n",
    "# Nom de la base de donn√©es\n",
    "DATABASE_NAME = \"eau_potable\"\n",
    "\n",
    "print(f\"üì¶ Base de donn√©es : {DATABASE_NAME}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da03c8de-bc5c-46ff-8321-3f5fcc0fba34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. V√©rification des fichiers Parquet dans BRONZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3454450-797e-45e3-be2c-0d95cb4a5be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã VISUALISATION DES TABLES DISPONIBLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# M√©thode 1 : Lister toutes les tables de la base de donn√©es\n",
    "print(f\"\\nüîç Tables dans la base '{DATABASE_NAME}' :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {DATABASE_NAME}\")\n",
    "    tables_df.show(truncate=False)\n",
    "    \n",
    "    # Compter le nombre de tables\n",
    "    nb_tables = tables_df.count()\n",
    "    print(f\"\\n‚úÖ Nombre total de tables : {nb_tables}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la r√©cup√©ration des tables : {str(e)}\")\n",
    "\n",
    "# M√©thode 2 : Afficher uniquement les noms des tables\n",
    "print(f\"\\nüìù Liste des noms de tables :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    tables = spark.catalog.listTables(DATABASE_NAME)\n",
    "    \n",
    "    for table in tables:\n",
    "        print(f\"  ‚úì {table.name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# M√©thode 3 : V√©rifier l'existence des tables sp√©cifiques\n",
    "print(f\"\\nüîé V√©rification des tables Bronze attendues :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "expected_tables = [\"dis_plv\", \"dis_result\"]\n",
    "\n",
    "for table_name in expected_tables:\n",
    "    try:\n",
    "        exists = spark.catalog.tableExists(f\"{DATABASE_NAME}.{table_name}\")\n",
    "        if exists:\n",
    "            # Compter le nombre de lignes\n",
    "            count = spark.sql(f\"SELECT COUNT(*) as count FROM {DATABASE_NAME}.{table_name}\").first()['count']\n",
    "            print(f\"  ‚úÖ {table_name} : existe ({count:,} lignes)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {table_name} : n'existe pas\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  {table_name} : erreur - {str(e)}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50cc14d-1ae9-462c-a625-acbe5c5da85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## visu des data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b054fd4-e500-4983-b3d7-1f6bec4136bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üëÄ APER√áU DES DONN√âES - 10 PREMI√àRES LIGNES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Table 1 : dis_plv\n",
    "print(f\"\\nüìä Table : dis_plv (1,932,481 lignes)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {DATABASE_NAME}.dis_plv\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=30)\n",
    "    \n",
    "    print(\"‚úÖ Aper√ßu dis_plv affich√©\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Table 2 : dis_result\n",
    "print(f\"\\nüìä Table : dis_result (59,568,134 lignes)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {DATABASE_NAME}.dis_result\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=30)\n",
    "    \n",
    "    print(\"‚úÖ Aper√ßu dis_result affich√©\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Bonus : Sch√©ma des tables\n",
    "print(f\"\\nüìã SCH√âMA DES TABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîµ Sch√©ma de dis_plv :\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    spark.sql(f\"DESCRIBE {DATABASE_NAME}.dis_plv\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(f\"\\nüü¢ Sch√©ma de dis_result :\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    spark.sql(f\"DESCRIBE {DATABASE_NAME}.dis_result\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b3527a-b285-4715-82ec-619d08a9d01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; select * from `eau_potable`.`dis_plv` limit 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5955a998-b128-4961-8ca5-cfa33961de21",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761741062311}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; select * from `eau_potable`.`dis_result` limit 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d038028e-bd0e-4d05-a42d-19bc1c25a5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## analyse des distict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5e4916-c05c-486a-b00e-ce7af01d34a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"üîç ANALYSE DES VALEURS DISTINCTES\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # ============================================================\n",
    "# # TABLE DIS_PLV - Colonnes de Conformit√©\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\nüìä TABLE : dis_plv\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Colonne 1 : plvconformitebacterio\n",
    "# print(f\"\\nüîµ Colonne : plvconformitebacterio\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             plvconformitebacterio,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "#         GROUP BY plvconformitebacterio\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 2 : plvconformitechimique\n",
    "# print(f\"\\nüîµ Colonne : plvconformitechimique\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             plvconformitechimique,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "#         GROUP BY plvconformitechimique\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 3 : plvconformitereferencebact\n",
    "# print(f\"\\nüîµ Colonne : plvconformitereferencebact\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             plvconformitereferencebact,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "#         GROUP BY plvconformitereferencebact\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # TABLE DIS_RESULT - Colonnes d'Analyse\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\n\\nüìä TABLE : dis_result\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Colonne 4 : cdparametresiseeaux\n",
    "# print(f\"\\nüü¢ Colonne : cdparametresiseeaux (TOP 20)\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             cdparametresiseeaux,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#         GROUP BY cdparametresiseeaux\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#         LIMIT 20\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 5 : libmajparametre\n",
    "# print(f\"\\nüü¢ Colonne : libmajparametre (TOP 30)\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             libmajparametre,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#         GROUP BY libmajparametre\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#         LIMIT 30\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 6 : libminparametre\n",
    "# print(f\"\\nüü¢ Colonne : libminparametre (TOP 30)\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             libminparametre,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#         WHERE libminparametre IS NOT NULL\n",
    "#         GROUP BY libminparametre\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#         LIMIT 30\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 7 : qualitparam\n",
    "# print(f\"\\nüü¢ Colonne : qualitparam\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             qualitparam,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#         GROUP BY qualitparam\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # Colonne 8 : insituana\n",
    "# print(f\"\\nüü¢ Colonne : insituana\")\n",
    "# print(\"-\" * 60)\n",
    "# try:\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             insituana,\n",
    "#             COUNT(*) as nb_occurrences,\n",
    "#             ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#         GROUP BY insituana\n",
    "#         ORDER BY nb_occurrences DESC\n",
    "#     \"\"\").show(truncate=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # STATISTIQUES GLOBALES\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\n\\nüìà STATISTIQUES GLOBALES\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# print(f\"\\nüîπ Nombre total de valeurs distinctes par colonne :\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# try:\n",
    "#     print(f\"\\nüìã TABLE dis_plv :\")\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             'plvconformitebacterio' as colonne,\n",
    "#             COUNT(DISTINCT plvconformitebacterio) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'plvconformitechimique' as colonne,\n",
    "#             COUNT(DISTINCT plvconformitechimique) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'plvconformitereferencebact' as colonne,\n",
    "#             COUNT(DISTINCT plvconformitereferencebact) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_plv\n",
    "#     \"\"\").show(truncate=False)\n",
    "    \n",
    "#     print(f\"\\nüìã TABLE dis_result :\")\n",
    "#     spark.sql(f\"\"\"\n",
    "#         SELECT \n",
    "#             'cdparametresiseeaux' as colonne,\n",
    "#             COUNT(DISTINCT cdparametresiseeaux) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'libmajparametre' as colonne,\n",
    "#             COUNT(DISTINCT libmajparametre) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'libminparametre' as colonne,\n",
    "#             COUNT(DISTINCT libminparametre) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'qualitparam' as colonne,\n",
    "#             COUNT(DISTINCT qualitparam) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "        \n",
    "#         UNION ALL\n",
    "        \n",
    "#         SELECT \n",
    "#             'insituana' as colonne,\n",
    "#             COUNT(DISTINCT insituana) as nb_valeurs_distinctes\n",
    "#         FROM {DATABASE_NAME}.dis_result\n",
    "#     \"\"\").show(truncate=False)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"‚úÖ Analyse des valeurs distinctes termin√©e\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020dfc83-8ff7-49c6-bc76-f6f729220cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ CR√âATION DE LA TABLE SILVER : silver_plv_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7902544-133b-4159-878e-6e116ff0196f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß CR√âATION TABLE SILVER : silver_plv_clean_2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, year, month, quarter, \n",
    "    when, trim, regexp_replace, concat, lit,\n",
    "    date_format, dayofweek, coalesce\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 1 : LECTURE DES DONN√âES BRONZE\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìñ Lecture de la table Bronze : dis_plv\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(f\"{DATABASE_NAME}.dis_plv\")\n",
    "    nb_lignes_bronze = df_bronze.count()\n",
    "    print(f\"‚úÖ Donn√©es charg√©es : {nb_lignes_bronze:,} lignes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de lecture : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 2 : NETTOYAGE ET TYPAGE\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüßπ Nettoyage et Typage des donn√©es...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_clean = df_bronze\n",
    "\n",
    "# 2.1 - Convertir dateprel en DATE\n",
    "print(\"  ‚û§ Conversion dateprel (string ‚Üí DATE)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"dateprel_clean\",\n",
    "    to_date(col(\"dateprel\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# 2.2 - Extraire l'heure de heureprel (ex: \"12h35\" ‚Üí 12)\n",
    "print(\"  ‚û§ Extraction heure (string '12h35' ‚Üí INT 12)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"heure_prel\",\n",
    "    regexp_replace(col(\"heureprel\"), \"h.*\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 2.3 - Convertir annee en INT\n",
    "print(\"  ‚û§ Conversion annee (string ‚Üí INT)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"annee_int\",\n",
    "    col(\"annee\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 2.4 - Nettoyer pourcentdebit (\"100 %\" ‚Üí 100.0)\n",
    "print(\"  ‚û§ Nettoyage pourcentdebit ('100 %' ‚Üí 100.0)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"pourcentdebit_clean\",\n",
    "    when(\n",
    "        col(\"pourcentdebit\").isNotNull(),\n",
    "        regexp_replace(col(\"pourcentdebit\"), \" %\", \"\").cast(\"float\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Nettoyage et typage termin√©s\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 3 : ENRICHISSEMENT TEMPOREL\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìÖ Enrichissement temporel...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 3.1 - Extraire ann√©e, mois, trimestre, semestre\n",
    "print(\"  ‚û§ Extraction : ann√©e, mois, trimestre, semestre\")\n",
    "df_clean = df_clean.withColumn(\"annee_prel\", year(col(\"dateprel_clean\")))\n",
    "df_clean = df_clean.withColumn(\"mois_prel\", month(col(\"dateprel_clean\")))\n",
    "df_clean = df_clean.withColumn(\"trimestre_prel\", quarter(col(\"dateprel_clean\")))\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"semestre_prel\",\n",
    "    when(col(\"mois_prel\") <= 6, 1).otherwise(2)\n",
    ")\n",
    "\n",
    "# 3.2 - Jour de la semaine (1=Dimanche, 2=Lundi, ..., 7=Samedi)\n",
    "print(\"  ‚û§ Calcul du jour de la semaine\")\n",
    "df_clean = df_clean.withColumn(\"jour_semaine_num\", dayofweek(col(\"dateprel_clean\")))\n",
    "\n",
    "# Mapper les num√©ros aux noms de jours\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"jour_semaine\",\n",
    "    when(col(\"jour_semaine_num\") == 1, \"Dimanche\")\n",
    "    .when(col(\"jour_semaine_num\") == 2, \"Lundi\")\n",
    "    .when(col(\"jour_semaine_num\") == 3, \"Mardi\")\n",
    "    .when(col(\"jour_semaine_num\") == 4, \"Mercredi\")\n",
    "    .when(col(\"jour_semaine_num\") == 5, \"Jeudi\")\n",
    "    .when(col(\"jour_semaine_num\") == 6, \"Vendredi\")\n",
    "    .when(col(\"jour_semaine_num\") == 7, \"Samedi\")\n",
    "    .otherwise(\"Inconnu\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enrichissement temporel termin√©\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 4 : TRAITEMENT DES VALEURS NULL\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîÑ Traitement des valeurs NULL...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Compter les NULL avant traitement\n",
    "null_count_before = df_clean.filter(\n",
    "    col(\"cdreseauamont\").isNull() | \n",
    "    col(\"nomreseauamont\").isNull() | \n",
    "    col(\"pourcentdebit_clean\").isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"  ‚ÑπÔ∏è  Lignes avec NULL avant : {null_count_before:,}\")\n",
    "\n",
    "# Remplacer les NULL\n",
    "print(\"  ‚û§ Remplacement des NULL par 'NON RENSEIGNE'\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"cdreseauamont\",\n",
    "    coalesce(col(\"cdreseauamont\"), lit(\"NON RENSEIGNE\"))\n",
    ")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"nomreseauamont\",\n",
    "    coalesce(col(\"nomreseauamont\"), lit(\"NON RENSEIGNE\"))\n",
    ")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"pourcentdebit_clean\",\n",
    "    coalesce(col(\"pourcentdebit_clean\"), lit(0.0))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Traitement des NULL termin√©\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 5 : S√âLECTION ET R√âORGANISATION DES COLONNES\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìã S√©lection des colonnes finales...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_silver = df_clean.select(\n",
    "    # Identifiants\n",
    "    col(\"cddept\"),\n",
    "    col(\"cdreseau\"),\n",
    "    col(\"inseecommuneprinc\"),\n",
    "    col(\"nomcommuneprinc\"),\n",
    "    col(\"cdreseauamont\"),\n",
    "    col(\"nomreseauamont\"),\n",
    "    \n",
    "    # Pr√©l√®vement\n",
    "    col(\"referenceprel\"),\n",
    "    col(\"dateprel_clean\").alias(\"dateprel\"),\n",
    "    col(\"heure_prel\"),\n",
    "    col(\"pourcentdebit_clean\").alias(\"pourcentdebit\"),\n",
    "    \n",
    "    # Enrichissement temporel\n",
    "    col(\"annee_int\").alias(\"annee\"),\n",
    "    col(\"annee_prel\"),\n",
    "    col(\"mois_prel\"),\n",
    "    col(\"trimestre_prel\"),\n",
    "    col(\"semestre_prel\"),\n",
    "    col(\"jour_semaine\"),\n",
    "    \n",
    "    # Conclusions et conformit√©\n",
    "    col(\"conclusionprel\"),\n",
    "    col(\"plvconformitebacterio\"),\n",
    "    col(\"plvconformitechimique\"),\n",
    "    col(\"plvconformitereferencebact\"),\n",
    "    col(\"plvconformitereferencechim\"),\n",
    "    \n",
    "    # Organismes\n",
    "    col(\"ugelib\"),\n",
    "    col(\"distrlib\"),\n",
    "    col(\"moalib\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Colonnes s√©lectionn√©es\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 6 : D√âDOUBLONNAGE\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîç D√©doublonnage des donn√©es...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "nb_lignes_avant = df_silver.count()\n",
    "print(f\"  ‚ÑπÔ∏è  Lignes avant d√©doublonnage : {nb_lignes_avant:,}\")\n",
    "\n",
    "# Identifier les doublons\n",
    "df_doublons = df_silver.groupBy(df_silver.columns).count().filter(col(\"count\") > 1)\n",
    "nb_doublons = df_doublons.count()\n",
    "print(f\"  ‚ö†Ô∏è  Doublons d√©tect√©s : {nb_doublons:,}\")\n",
    "\n",
    "if nb_doublons > 0:\n",
    "    print(f\"\\n  üìä Aper√ßu des doublons (TOP 5) :\")\n",
    "    df_doublons.orderBy(col(\"count\").desc()).show(5, truncate=40)\n",
    "\n",
    "# Supprimer les doublons\n",
    "df_silver_unique = df_silver.dropDuplicates()\n",
    "nb_lignes_apres = df_silver_unique.count()\n",
    "\n",
    "print(f\"  ‚úÖ Lignes apr√®s d√©doublonnage : {nb_lignes_apres:,}\")\n",
    "print(f\"  ‚û§ Lignes supprim√©es : {nb_lignes_avant - nb_lignes_apres:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 7 : CR√âATION DE LA TABLE SILVER\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüíæ Cr√©ation de la table Silver...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "table_name = f\"{DATABASE_NAME}.silver_plv_clean_2\"\n",
    "\n",
    "try:\n",
    "    # Supprimer la table si elle existe d√©j√†\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"  ‚ÑπÔ∏è  Table existante supprim√©e : {table_name}\")\n",
    "    \n",
    "    # Cr√©er la table\n",
    "    df_silver_unique.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    print(f\"‚úÖ Table cr√©√©e avec succ√®s : {table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la cr√©ation : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 8 : V√âRIFICATION ET STATISTIQUES\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìä Statistiques finales\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compter les lignes dans la table\n",
    "nb_lignes_table = spark.table(table_name).count()\n",
    "print(f\"‚úÖ Lignes dans la table : {nb_lignes_table:,}\")\n",
    "\n",
    "# Afficher le sch√©ma\n",
    "print(f\"\\nüìã Sch√©ma de la table :\")\n",
    "print(\"-\" * 60)\n",
    "spark.table(table_name).printSchema()\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(f\"\\nüëÄ Aper√ßu des 10 premi√®res lignes :\")\n",
    "print(\"-\" * 60)\n",
    "spark.table(table_name).show(10, truncate=30)\n",
    "\n",
    "# Statistiques par ann√©e\n",
    "print(f\"\\nüìà R√©partition par ann√©e :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        annee,\n",
    "        COUNT(*) as nb_prelevements,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY annee\n",
    "    ORDER BY annee DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Statistiques de conformit√©\n",
    "print(f\"\\nüéØ Statistiques de conformit√© :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        plvconformitebacterio,\n",
    "        plvconformitechimique,\n",
    "        COUNT(*) as nb_prelevements\n",
    "    FROM {table_name}\n",
    "    GROUP BY plvconformitebacterio, plvconformitechimique\n",
    "    ORDER BY nb_prelevements DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ TABLE SILVER silver_plv_clean_2 CR√â√âE AVEC SUCC√àS !\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874e0cd1-7cbf-4aec-883a-692e833a031d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287ddc1e-ea08-4a5e-a49f-85dead279a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"üóëÔ∏è  SUPPRESSION DE LA TABLE silver_plv_clean\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # ============================================================\n",
    "# # √âTAPE 1 : SUPPRESSION DE LA VUE TEMPORAIRE\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\nüîπ √âTAPE 1 : Suppression de la vue temporaire\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# try:\n",
    "#     # V√©rifier si la vue temporaire existe\n",
    "#     temp_tables = [table.name for table in spark.catalog.listTables(\"default\") if table.isTemporary]\n",
    "    \n",
    "#     if \"silver_plv_clean\" in temp_tables:\n",
    "#         spark.catalog.dropTempView(\"silver_plv_clean\")\n",
    "#         print(\"‚úÖ Vue temporaire 'silver_plv_clean' supprim√©e\")\n",
    "#     else:\n",
    "#         print(\"‚ÑπÔ∏è  Aucune vue temporaire 'silver_plv_clean' trouv√©e\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è  Erreur lors de la suppression de la vue temporaire : {str(e)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # √âTAPE 2 : SUPPRESSION DE LA TABLE DELTA (si elle existe)\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\nüîπ √âTAPE 2 : Suppression de la table Delta Lake (si elle existe)\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# try:\n",
    "#     # V√©rifier si la table existe dans le catalogue\n",
    "#     table_exists = spark.catalog.tableExists(f\"{DATABASE_NAME}.silver_plv_clean\")\n",
    "    \n",
    "#     if table_exists:\n",
    "#         spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_NAME}.silver_plv_clean\")\n",
    "#         print(f\"‚úÖ Table '{DATABASE_NAME}.silver_plv_clean' supprim√©e du catalogue\")\n",
    "#     else:\n",
    "#         print(f\"‚ÑπÔ∏è  Aucune table '{DATABASE_NAME}.silver_plv_clean' trouv√©e dans le catalogue\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è  Erreur lors de la suppression de la table : {str(e)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # √âTAPE 3 : V√âRIFICATION\n",
    "# # ============================================================\n",
    "\n",
    "# print(f\"\\nüîπ √âTAPE 3 : V√©rification de la suppression\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# try:\n",
    "#     # V√©rifier les vues temporaires\n",
    "#     temp_tables = [table.name for table in spark.catalog.listTables(\"default\") if table.isTemporary]\n",
    "    \n",
    "#     if \"silver_plv_clean\" not in temp_tables:\n",
    "#         print(\"‚úÖ Vue temporaire supprim√©e avec succ√®s\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è  La vue temporaire existe toujours\")\n",
    "    \n",
    "#     # V√©rifier les tables dans la base de donn√©es\n",
    "#     table_exists = spark.catalog.tableExists(f\"{DATABASE_NAME}.silver_plv_clean\")\n",
    "    \n",
    "#     if not table_exists:\n",
    "#         print(f\"‚úÖ Table Delta supprim√©e avec succ√®s\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è  La table existe toujours dans {DATABASE_NAME}\")\n",
    "    \n",
    "#     # Lister les tables actuelles dans la base\n",
    "#     print(f\"\\nüìã Tables actuelles dans '{DATABASE_NAME}' :\")\n",
    "#     spark.sql(f\"SHOW TABLES IN {DATABASE_NAME}\").show(truncate=False)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Erreur lors de la v√©rification : {str(e)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # R√âSUM√â\n",
    "# # ============================================================\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"‚úÖ OP√âRATION DE SUPPRESSION TERMIN√âE\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"üóëÔ∏è  Vue temporaire : v√©rifi√©e\")\n",
    "# print(\"üóëÔ∏è  Table Delta : v√©rifi√©e\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e7ba5b-4435-4dd6-b094-ead234a77022",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761748611929}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; use schema `eau_potable`; select * from `eau_potable`.`silver_plv_clean_2` limit 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fea592c-cabd-494f-88cd-aac7b1782352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; select * from `eau_potable`.`dis_result` limit 45;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de854776-ad02-42fb-8c1f-26a011d2d28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# silver result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be318b1c-25c2-4fc3-b79d-c6697e0d64bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß CR√âATION TABLE SILVER : silver_result_clean_2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, trim, regexp_replace, lit, coalesce, upper\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 1 : LECTURE DES DONN√âES BRONZE\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìñ Lecture de la table Bronze : dis_result\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(f\"{DATABASE_NAME}.dis_result\")\n",
    "    nb_lignes_bronze = df_bronze.count()\n",
    "    print(f\"‚úÖ Donn√©es charg√©es : {nb_lignes_bronze:,} lignes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de lecture : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 2 : CONVERSION DES TYPES\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîÑ Conversion des types...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_clean = df_bronze\n",
    "\n",
    "# 2.1 - Convertir annee en INT\n",
    "print(\"  ‚û§ Conversion annee (string ‚Üí INT)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"annee_int\",\n",
    "    col(\"annee\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# 2.2 - Convertir valtraduite en FLOAT\n",
    "print(\"  ‚û§ Conversion valtraduite (string ‚Üí FLOAT)\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"valtraduite_float\",\n",
    "    col(\"valtraduite\").cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversion des types termin√©e\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 3 : CAT√âGORISATION DES PARAM√àTRES\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Cat√©gorisation des param√®tres...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 3.1 - Cr√©er categorie_parametre\n",
    "print(\"  ‚û§ Cr√©ation de categorie_parametre\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"categorie_parametre\",\n",
    "    when(col(\"cdparametre\").isin(\n",
    "        \"ECOLI\", \"CTF\", \"STRF\", \"BSIR\", \"GT22_68\", \"GT36_44\"\n",
    "    ), \"BACTERIOLOGIE\")\n",
    "    .when(col(\"cdparametre\").isin(\n",
    "        \"PH\", \"CDT25\", \"TEAU\", \"TURBNFU\", \"COULF\", \"COULQ\"\n",
    "    ), \"PHYSICO_CHIMIE\")\n",
    "    .when(col(\"cdparametre\").isin(\n",
    "        \"NH4\", \"NO2\", \"NO3\", \"CL\", \"SO4\", \"CA\", \"MG\", \"NO3_NO2\"\n",
    "    ), \"CHIMIE_MINERALE\")\n",
    "    .when(col(\"cdparametre\").isin(\n",
    "        \"CL2LIB\", \"CL2TOT\", \"O3\", \"CLO2\"\n",
    "    ), \"DESINFECTANTS\")\n",
    "    .when(col(\"cdparametre\").isin(\n",
    "        \"COT\", \"CLVYL\"\n",
    "    ), \"CHIMIE_ORGANIQUE\")\n",
    "    .when(col(\"cdparametre\").isin(\n",
    "        \"ASP\", \"ODQ\", \"SAVQ\"\n",
    "    ), \"QUALITATIF\")\n",
    "    .when(col(\"libmajparametre\").rlike(\"(?i)(PLOMB|CUIVRE|FER|ZINC|NICKEL|CHROME|CADMIUM|MERCURE|ARSENIC|ALUMINIUM)\"), \"METAUX\")\n",
    "    .when(col(\"libmajparametre\").rlike(\"(?i)(PESTICIDE|HERBICIDE|INSECTICIDE|ATRAZINE|SIMAZINE)\"), \"PESTICIDES\")\n",
    "    .when(col(\"cdparametre\").isin(\"TA\", \"TAC\", \"TH\"), \"DURETE_ALCALINITE\")\n",
    "    .otherwise(\"AUTRES\")\n",
    ")\n",
    "\n",
    "# 3.2 - Cr√©er sous_categorie\n",
    "print(\"  ‚û§ Cr√©ation de sous_categorie\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"sous_categorie\",\n",
    "    # BACTERIOLOGIE\n",
    "    when(col(\"cdparametre\") == \"ECOLI\", \"ECOLI\")\n",
    "    .when(col(\"cdparametre\").isin(\"CTF\"), \"COLIFORMES\")\n",
    "    .when(col(\"cdparametre\") == \"STRF\", \"ENTEROCOQUES\")\n",
    "    .when(col(\"cdparametre\") == \"BSIR\", \"SPORES\")\n",
    "    .when(col(\"cdparametre\").isin(\"GT22_68\", \"GT36_44\"), \"AEROBIES\")\n",
    "    \n",
    "    # CHIMIE_MINERALE\n",
    "    .when(col(\"cdparametre\").isin(\"NO3\", \"NO3_NO2\"), \"NITRATES\")\n",
    "    .when(col(\"cdparametre\") == \"NO2\", \"NITRITES\")\n",
    "    .when(col(\"cdparametre\") == \"NH4\", \"AMMONIUM\")\n",
    "    .when(col(\"cdparametre\") == \"CL\", \"CHLORURES\")\n",
    "    .when(col(\"cdparametre\") == \"SO4\", \"SULFATES\")\n",
    "    .when(col(\"cdparametre\") == \"CA\", \"CALCIUM\")\n",
    "    .when(col(\"cdparametre\") == \"MG\", \"MAGNESIUM\")\n",
    "    \n",
    "    # PHYSICO_CHIMIE\n",
    "    .when(col(\"cdparametre\") == \"PH\", \"PH\")\n",
    "    .when(col(\"cdparametre\") == \"CDT25\", \"CONDUCTIVITE\")\n",
    "    .when(col(\"cdparametre\") == \"TEAU\", \"TEMPERATURE\")\n",
    "    .when(col(\"cdparametre\") == \"TURBNFU\", \"TURBIDITE\")\n",
    "    .when(col(\"cdparametre\").isin(\"COULF\", \"COULQ\"), \"COULEUR\")\n",
    "    \n",
    "    # DESINFECTANTS\n",
    "    .when(col(\"cdparametre\") == \"CL2LIB\", \"CHLORE_LIBRE\")\n",
    "    .when(col(\"cdparametre\") == \"CL2TOT\", \"CHLORE_TOTAL\")\n",
    "    .when(col(\"cdparametre\") == \"O3\", \"OZONE\")\n",
    "    .when(col(\"cdparametre\") == \"CLO2\", \"DIOXYDE_CHLORE\")\n",
    "    \n",
    "    # CHIMIE_ORGANIQUE\n",
    "    .when(col(\"cdparametre\") == \"COT\", \"CARBONE_ORGANIQUE\")\n",
    "    .when(col(\"cdparametre\") == \"CLVYL\", \"CHLORURE_VINYLE\")\n",
    "    \n",
    "    # QUALITATIF\n",
    "    .when(col(\"cdparametre\") == \"ASP\", \"ASPECT\")\n",
    "    .when(col(\"cdparametre\") == \"ODQ\", \"ODEUR\")\n",
    "    .when(col(\"cdparametre\") == \"SAVQ\", \"SAVEUR\")\n",
    "    \n",
    "    # DURETE_ALCALINITE\n",
    "    .when(col(\"cdparametre\") == \"TA\", \"TITRE_ALCALIMETRIQUE\")\n",
    "    .when(col(\"cdparametre\") == \"TAC\", \"TITRE_ALCALIMETRIQUE_COMPLET\")\n",
    "    .when(col(\"cdparametre\") == \"TH\", \"TITRE_HYDROTIMETRIQUE\")\n",
    "    \n",
    "    .otherwise(\"AUTRE\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Cat√©gorisation termin√©e\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 4 : S√âLECTION DES COLONNES (SUPPRESSION libminparametre)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìã S√©lection des colonnes finales...\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  ‚û§ Suppression de la colonne : libminparametre\")\n",
    "\n",
    "df_silver = df_clean.select(\n",
    "    # Identifiants\n",
    "    col(\"cddept\"),\n",
    "    col(\"referenceprel\"),\n",
    "    col(\"cdparametresiseeaux\"),\n",
    "    col(\"cdparametre\"),\n",
    "    \n",
    "    # Param√®tre (libminparametre supprim√©e)\n",
    "    col(\"libmajparametre\"),\n",
    "    col(\"libwebparametre\"),\n",
    "    \n",
    "    # Cat√©gorisation (nouvelles colonnes)\n",
    "    col(\"categorie_parametre\"),\n",
    "    col(\"sous_categorie\"),\n",
    "    \n",
    "    # Qualit√© et type\n",
    "    col(\"qualitparam\"),\n",
    "    col(\"insituana\"),\n",
    "    \n",
    "    # R√©sultats\n",
    "    col(\"rqana\"),\n",
    "    col(\"valtraduite_float\").alias(\"valtraduite\"),\n",
    "    \n",
    "    # Unit√©s\n",
    "    col(\"cdunitereferencesiseeaux\"),\n",
    "    col(\"cdunitereference\"),\n",
    "    \n",
    "    # Limites (conserv√©es brutes)\n",
    "    col(\"limitequal\"),\n",
    "    col(\"refqual\"),\n",
    "    \n",
    "    # Autres\n",
    "    col(\"casparam\"),\n",
    "    col(\"referenceanl\"),\n",
    "    \n",
    "    # Ann√©e\n",
    "    col(\"annee_int\").alias(\"annee\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Colonnes s√©lectionn√©es (libminparametre supprim√©e)\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 5 : D√âDOUBLONNAGE\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîç D√©doublonnage des donn√©es...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "nb_lignes_avant = df_silver.count()\n",
    "print(f\"  ‚ÑπÔ∏è  Lignes avant d√©doublonnage : {nb_lignes_avant:,}\")\n",
    "\n",
    "# Identifier les doublons\n",
    "df_doublons = df_silver.groupBy(df_silver.columns).count().filter(col(\"count\") > 1)\n",
    "nb_doublons = df_doublons.count()\n",
    "print(f\"  ‚ö†Ô∏è  Doublons d√©tect√©s : {nb_doublons:,}\")\n",
    "\n",
    "if nb_doublons > 0:\n",
    "    print(f\"\\n  üìä Aper√ßu des doublons (TOP 5) :\")\n",
    "    df_doublons.select(\"referenceprel\", \"cdparametre\", \"valtraduite\", \"count\") \\\n",
    "        .orderBy(col(\"count\").desc()).show(5, truncate=40)\n",
    "\n",
    "# Supprimer les doublons\n",
    "df_silver_unique = df_silver.dropDuplicates()\n",
    "nb_lignes_apres = df_silver_unique.count()\n",
    "\n",
    "print(f\"  ‚úÖ Lignes apr√®s d√©doublonnage : {nb_lignes_apres:,}\")\n",
    "print(f\"  ‚û§ Lignes supprim√©es : {nb_lignes_avant - nb_lignes_apres:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 6 : CR√âATION DE LA TABLE SILVER\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüíæ Cr√©ation de la table Silver...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "table_name = f\"{DATABASE_NAME}.silver_result_clean_2\"\n",
    "\n",
    "try:\n",
    "    # Supprimer la table si elle existe d√©j√†\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"  ‚ÑπÔ∏è  Table existante supprim√©e : {table_name}\")\n",
    "    \n",
    "    # Cr√©er la table (partitionn√©e par annee pour performance)\n",
    "    df_silver_unique.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .partitionBy(\"annee\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    print(f\"‚úÖ Table cr√©√©e avec succ√®s : {table_name}\")\n",
    "    print(f\"  ‚û§ Partitionnement : par annee\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la cr√©ation : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 7 : V√âRIFICATION ET STATISTIQUES\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìä Statistiques finales\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compter les lignes dans la table\n",
    "nb_lignes_table = spark.table(table_name).count()\n",
    "print(f\"‚úÖ Lignes dans la table : {nb_lignes_table:,}\")\n",
    "\n",
    "# Afficher le sch√©ma\n",
    "print(f\"\\nüìã Sch√©ma de la table :\")\n",
    "print(\"-\" * 60)\n",
    "spark.table(table_name).printSchema()\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(f\"\\nüëÄ Aper√ßu des 10 premi√®res lignes :\")\n",
    "print(\"-\" * 60)\n",
    "spark.table(table_name).show(10, truncate=30)\n",
    "\n",
    "# Statistiques par cat√©gorie\n",
    "print(f\"\\nüìà R√©partition par cat√©gorie de param√®tre :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        categorie_parametre,\n",
    "        COUNT(*) as nb_analyses,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY categorie_parametre\n",
    "    ORDER BY nb_analyses DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Statistiques par ann√©e\n",
    "print(f\"\\nüìÖ R√©partition par ann√©e :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        annee,\n",
    "        COUNT(*) as nb_analyses,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY annee\n",
    "    ORDER BY annee DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Top 10 param√®tres\n",
    "print(f\"\\nüîù Top 10 param√®tres analys√©s :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        libmajparametre,\n",
    "        categorie_parametre,\n",
    "        sous_categorie,\n",
    "        COUNT(*) as nb_analyses\n",
    "    FROM {table_name}\n",
    "    GROUP BY libmajparametre, categorie_parametre, sous_categorie\n",
    "    ORDER BY nb_analyses DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=40)\n",
    "\n",
    "# R√©partition qualitparam\n",
    "print(f\"\\nüî¨ R√©partition par type de param√®tre (Num√©rique/Qualitatif) :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN qualitparam = 'N' THEN 'NUMERIQUE'\n",
    "            WHEN qualitparam = 'O' THEN 'QUALITATIF'\n",
    "            ELSE 'AUTRE'\n",
    "        END as type_parametre,\n",
    "        COUNT(*) as nb_analyses,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY qualitparam\n",
    "    ORDER BY nb_analyses DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# R√©partition insituana\n",
    "print(f\"\\nüè¢ R√©partition par lieu d'analyse :\")\n",
    "print(\"-\" * 60)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN insituana = 'L' THEN 'LABORATOIRE'\n",
    "            WHEN insituana = 'T' THEN 'TERRAIN'\n",
    "            ELSE 'AUTRE'\n",
    "        END as lieu_analyse,\n",
    "        COUNT(*) as nb_analyses,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY insituana\n",
    "    ORDER BY nb_analyses DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ TABLE SILVER silver_result_clean_2 CR√â√âE AVEC SUCC√àS !\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a12ab46-1e3a-4a6f-8bd9-170b3917272d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761750535793}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; select * from `eau_potable`.`silver_result_clean_2` limit 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5aef069-9b1e-49b1-9b18-d9d7588a628d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì§ EXPORT DES TABLES SILVER VERS DATALAKE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 1 : CONFIGURATION DES CHEMINS\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîß Configuration des chemins d'export...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Nom du conteneur cible\n",
    "container_name = \"silver\"\n",
    "\n",
    "# Construire le chemin de base\n",
    "datalake_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# Chemins pour chaque table\n",
    "path_silver_plv = f\"{datalake_path}/silver_plv_clean_2\"\n",
    "path_silver_result = f\"{datalake_path}/silver_result_clean_2\"\n",
    "\n",
    "print(f\"‚úÖ Conteneur cible : {container_name}\")\n",
    "print(f\"‚úÖ Chemin silver_plv_clean_2 : {path_silver_plv}\")\n",
    "print(f\"‚úÖ Chemin silver_result_clean_2 : {path_silver_result}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 2 : EXPORT silver_plv_clean_2\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüì¶ Export de silver_plv_clean_2...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lire la table depuis le metastore\n",
    "    df_plv = spark.table(f\"{DATABASE_NAME}.silver_plv_clean_2\")\n",
    "    nb_lignes_plv = df_plv.count()\n",
    "    print(f\"  ‚ÑπÔ∏è  Lignes √† exporter : {nb_lignes_plv:,}\")\n",
    "    \n",
    "    # Coalesce √† 1 pour avoir UN SEUL fichier parquet\n",
    "    df_plv_single = df_plv.coalesce(1)\n",
    "    \n",
    "    # Exporter en parquet\n",
    "    df_plv_single.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(path_silver_plv)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Export silver_plv_clean_2 termin√© en {elapsed_time:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'export silver_plv_clean_2 : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 3 : EXPORT silver_result_clean_2\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüì¶ Export de silver_result_clean_2...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Lire la table depuis le metastore\n",
    "    df_result = spark.table(f\"{DATABASE_NAME}.silver_result_clean_2\")\n",
    "    nb_lignes_result = df_result.count()\n",
    "    print(f\"  ‚ÑπÔ∏è  Lignes √† exporter : {nb_lignes_result:,}\")\n",
    "    \n",
    "    # Coalesce √† 1 pour avoir UN SEUL fichier parquet\n",
    "    df_result_single = df_result.coalesce(1)\n",
    "    \n",
    "    # Exporter en parquet\n",
    "    df_result_single.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(path_silver_result)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Export silver_result_clean_2 termin√© en {elapsed_time:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'export silver_result_clean_2 : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 4 : V√âRIFICATION DES EXPORTS\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüîç V√©rification des exports...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# V√©rification silver_plv_clean_2\n",
    "print(f\"\\nüìÇ V√©rification : silver_plv_clean_2\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    df_plv_verify = spark.read.parquet(path_silver_plv)\n",
    "    nb_lignes_plv_verify = df_plv_verify.count()\n",
    "    nb_colonnes_plv = len(df_plv_verify.columns)\n",
    "    \n",
    "    print(f\"  ‚úÖ Fichier lu avec succ√®s\")\n",
    "    print(f\"  ‚úÖ Lignes : {nb_lignes_plv_verify:,}\")\n",
    "    print(f\"  ‚úÖ Colonnes : {nb_colonnes_plv}\")\n",
    "    \n",
    "    # V√©rifier que le nombre de lignes correspond\n",
    "    if nb_lignes_plv == nb_lignes_plv_verify:\n",
    "        print(f\"  ‚úÖ Coh√©rence v√©rifi√©e : {nb_lignes_plv:,} lignes\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Incoh√©rence : {nb_lignes_plv:,} ‚Üí {nb_lignes_plv_verify:,}\")\n",
    "    \n",
    "    # Afficher aper√ßu\n",
    "    print(f\"\\n  üëÄ Aper√ßu (5 premi√®res lignes) :\")\n",
    "    df_plv_verify.show(5, truncate=30)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur de v√©rification : {str(e)}\")\n",
    "\n",
    "# V√©rification silver_result_clean_2\n",
    "print(f\"\\nüìÇ V√©rification : silver_result_clean_2\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    df_result_verify = spark.read.parquet(path_silver_result)\n",
    "    nb_lignes_result_verify = df_result_verify.count()\n",
    "    nb_colonnes_result = len(df_result_verify.columns)\n",
    "    \n",
    "    print(f\"  ‚úÖ Fichier lu avec succ√®s\")\n",
    "    print(f\"  ‚úÖ Lignes : {nb_lignes_result_verify:,}\")\n",
    "    print(f\"  ‚úÖ Colonnes : {nb_colonnes_result}\")\n",
    "    \n",
    "    # V√©rifier que le nombre de lignes correspond\n",
    "    if nb_lignes_result == nb_lignes_result_verify:\n",
    "        print(f\"  ‚úÖ Coh√©rence v√©rifi√©e : {nb_lignes_result:,} lignes\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Incoh√©rence : {nb_lignes_result:,} ‚Üí {nb_lignes_result_verify:,}\")\n",
    "    \n",
    "    # Afficher aper√ßu\n",
    "    print(f\"\\n  üëÄ Aper√ßu (5 premi√®res lignes) :\")\n",
    "    df_result_verify.show(5, truncate=30)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur de v√©rification : {str(e)}\")\n",
    "\n",
    "# ============================================================\n",
    "# √âTAPE 5 : LISTER LES FICHIERS CR√â√âS\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nüìã Liste des fichiers cr√©√©s dans le datalake\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Lister fichiers silver_plv_clean_2\n",
    "print(f\"\\nüìÅ Dossier : silver_plv_clean_2\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    files_plv = dbutils.fs.ls(path_silver_plv)\n",
    "    for file in files_plv:\n",
    "        size_mb = file.size / (1024 * 1024)\n",
    "        print(f\"  üìÑ {file.name} - {size_mb:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Lister fichiers silver_result_clean_2\n",
    "print(f\"\\nüìÅ Dossier : silver_result_clean_2\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    files_result = dbutils.fs.ls(path_silver_result)\n",
    "    for file in files_result:\n",
    "        size_mb = file.size / (1024 * 1024)\n",
    "        print(f\"  üìÑ {file.name} - {size_mb:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# ============================================================\n",
    "# R√âSUM√â FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ EXPORT TERMIN√â AVEC SUCC√àS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä R√©sum√© de l'export :\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  ‚úÖ silver_plv_clean_2 : {nb_lignes_plv:,} lignes export√©es\")\n",
    "print(f\"  ‚úÖ silver_result_clean_2 : {nb_lignes_result:,} lignes export√©es\")\n",
    "print(f\"\\nüìç Localisation :\")\n",
    "print(f\"  üìÇ Conteneur : {container_name}\")\n",
    "print(f\"  üìÇ Storage Account : {storage_account_name}\")\n",
    "print(f\"\\nüì¶ Format : Parquet (1 fichier par table)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8376292268525949,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Silver_Transformation_azuredatabricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
