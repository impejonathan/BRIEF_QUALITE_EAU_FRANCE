name: Pipeline Qualit√© Eau - CI/CD
on:
  workflow_dispatch: {}

env:
  PYTHON_VERSION: '3.10'
  DATABRICKS_HOST: "https://${{ secrets.DATABRICKS_HOST }}"  # Ajout de "https://"
  DATABRICKS_TOKEN: ${{ secrets.TOKEN_AZUREDATABRICKS }}

jobs:
  # ============================================================
  # JOB 1 : ANALYSE DU CONTENEUR "RAW" (GitHub Actions)
  # ============================================================
  check-raw-files:
    name: "üîç √âtape 1 - Analyse du conteneur 'raw'"
    runs-on: ubuntu-latest
    outputs:
      has_zip: ${{ steps.check-raw.outputs.has_zip }}

    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üìö Installation des d√©pendances Azure
        run: |
          pip install azure-storage-blob

      - name: üîê Configuration des variables d'environnement
        run: |
          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV

      - name: üîç V√©rification des fichiers dans le conteneur "raw"
        id: check-raw
        run: |
          echo "================================"
          echo "üîç ANALYSE DU CONTENEUR 'RAW'"
          echo "================================"

          python3 << 'EOF'
          from azure.storage.blob import BlobServiceClient
          import os

          # R√©cup√©rer les variables d'environnement
          storage_account_name = os.environ.get("AZURE_STORAGE_ACCOUNT_NAME")
          storage_account_key = os.environ.get("AZURE_STORAGE_ACCOUNT_KEY")
          container_name = os.environ.get("CONTAINER_RAW")

          if not all([storage_account_name, storage_account_key, container_name]):
              raise ValueError("‚ùå Les variables d'environnement ne sont pas configur√©es")

          # Connexion au compte de stockage
          connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net"
          blob_service_client = BlobServiceClient.from_connection_string(connection_string)

          # Lister les fichiers dans le conteneur "raw"
          container_client = blob_service_client.get_container_client(container_name)
          blobs = list(container_client.list_blobs())

          if not blobs:
              print("‚ùå Aucun fichier trouv√© dans le conteneur 'raw'")
              print("::set-output name=has_zip::false")
          else:
              has_zip = any(blob.name.endswith('.zip') for blob in blobs)
              if has_zip:
                  print("‚úÖ Fichiers .zip trouv√©s dans 'raw' ‚Üí Passage direct √† l'√©tape 3")
                  print("::set-output name=has_zip::true")
              else:
                  print("‚ÑπÔ∏è  Fichiers trouv√©s, mais pas de .zip ‚Üí Ex√©cution de l'ingestion locale")
                  print("::set-output name=has_zip::false")
          EOF

  # ============================================================
  # JOB 2 : INGESTION LOCALE (GitHub Actions)
  # ============================================================
  ingestion-local:
    name: "üì• √âtape 2 - Ingestion Locale"
    needs: check-raw-files
    if: needs.check-raw-files.outputs.has_zip == 'false'
    runs-on: ubuntu-latest

    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üìö Installation des d√©pendances
        run: |
          echo "================================"
          echo "üìö INSTALLATION DES D√âPENDANCES"
          echo "================================"

          python -m pip install --upgrade pip

          if [ -f requirements.txt ]; then
            echo "üìÑ Installation depuis requirements.txt"
            pip install -r requirements.txt
          fi

          echo "‚òÅÔ∏è  Installation des packages Azure"
          pip install azure-storage-blob azure-identity azure-core

          echo "üìì Installation de papermill"
          pip install papermill nbformat nbconvert jupyter

          pip install python-dotenv

          echo "‚úÖ Toutes les d√©pendances sont install√©es"
          pip list | grep azure

      - name: üîê Configuration des variables d'environnement
        run: |
          echo "================================"
          echo "üîê CONFIGURATION DES SECRETS"
          echo "================================"

          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_CONNECTION_STRING=${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV
          echo "CONTAINER_BRONZE=${{ secrets.CONTAINER_BRONZE }}" >> $GITHUB_ENV

          echo "‚úÖ Variables d'environnement configur√©es"

      - name: üìù V√©rification du notebook
        run: |
          echo "================================"
          echo "üìù V√âRIFICATION DU NOTEBOOK"
          echo "================================"

          if [ ! -f "00_qualite_eau_ingestion.ipynb" ]; then
            echo "‚ùå Le fichier 00_qualite_eau_ingestion.ipynb n'existe pas"
            exit 1
          fi

          echo "‚úÖ Notebook trouv√© : 00_qualite_eau_ingestion.ipynb"

          python3 << EOF
          import json
          with open('00_qualite_eau_ingestion.ipynb', 'r', encoding='utf-8') as f:
              nb = json.load(f)
              print(f"üìä Nombre de cellules : {len(nb['cells'])}")
              print(f"üìä Kernel : {nb['metadata'].get('kernelspec', {}).get('name', 'N/A')}")
          EOF

      - name: üöÄ Ex√©cution du notebook d'ingestion
        run: |
          echo "================================"
          echo "üì• √âTAPE 2 : INGESTION DES DONN√âES"
          echo "================================"

          export AZURE_STORAGE_ACCOUNT_NAME="${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}"
          export AZURE_STORAGE_ACCOUNT_KEY="${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}"
          export AZURE_STORAGE_CONNECTION_STRING="${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}"
          export CONTAINER_RAW="${{ secrets.CONTAINER_RAW }}"
          export CONTAINER_BRONZE="${{ secrets.CONTAINER_BRONZE }}"

          papermill \
            00_qualite_eau_ingestion.ipynb \
            00_qualite_eau_ingestion_output.ipynb \
            --log-output \
            --progress-bar \
            --request-save-on-cell-execute

          echo ""
          echo "‚úÖ Ingestion termin√©e avec succ√®s"

      - name: üìä V√©rification des r√©sultats
        if: success()
        run: |
          echo "================================"
          echo "üìä V√âRIFICATION DES R√âSULTATS"
          echo "================================"

          echo "‚úÖ Notebook ex√©cut√© : 00_qualite_eau_ingestion.ipynb"
          echo "üìÑ Output g√©n√©r√© : 00_qualite_eau_ingestion_output.ipynb"

          if [ -f "00_qualite_eau_ingestion_output.ipynb" ]; then
            echo "‚úÖ Fichier output cr√©√© avec succ√®s"

            python3 << EOF
            import json
            with open('00_qualite_eau_ingestion_output.ipynb', 'r', encoding='utf-8') as f:
                nb = json.load(f)
                cells_executed = sum(1 for cell in nb['cells'] if cell.get('execution_count'))
                print(f"üìä Cellules ex√©cut√©es : {cells_executed}/{len(nb['cells'])}")
            EOF
          else
            echo "‚ö†Ô∏è  Fichier output non trouv√©"
          fi

      - name: üíæ Upload du notebook ex√©cut√© (artifact)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-ingestion-output
          path: |
            00_qualite_eau_ingestion_output.ipynb
            *.log
          retention-days: 7

      - name: ‚ùå Gestion des erreurs
        if: failure()
        run: |
          echo "================================"
          echo "‚ùå ERREUR LORS DE L'INGESTION"
          echo "================================"

          echo "üìÑ Consultez les logs ci-dessus pour plus de d√©tails"
          echo "üí° V√©rifiez :"
          echo "   - Les credentials Azure sont corrects"
          echo "   - Le container existe"
          echo "   - Les permissions sont configur√©es"
          exit 1

      - name: ‚úÖ R√©sum√© de l'ex√©cution
        if: success()
        run: |
          echo "================================"
          echo "‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS"
          echo "================================"

          echo "üìä R√©sum√© :"
          echo "   ‚úÖ Notebook ex√©cut√© : 00_qualite_eau_ingestion.ipynb"
          echo "   ‚úÖ Output disponible : 00_qualite_eau_ingestion_output.ipynb"
          echo "   ‚úÖ Artifacts upload√©s : notebook-ingestion-output"
          echo ""
          echo "üéâ √âtape 2 du pipeline compl√©t√©e !"

  # ============================================================
  # JOB 3 : EX√âCUTION DU PIPELINE "ETL v1" SUR DATABRICKS
  # ============================================================
  run-databricks-pipeline:
    name: "üöÄ √âtape 3 - Ex√©cution du pipeline 'ETL v1' sur Databricks"
    needs: check-raw-files
    if: needs.check-raw-files.outputs.has_zip == 'true' || (needs.check-raw-files.outputs.has_zip == 'false' && needs.ingestion-local.result == 'success')
    runs-on: ubuntu-latest

    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üöÄ Lancement du pipeline "ETL v1" sur Databricks
        run: |
          echo "================================"
          echo "üöÄ LANCEMENT DU PIPELINE 'ETL v1'"
          echo "================================"

          echo "DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}"
          echo "DATABRICKS_TOKEN: ${TOKEN_AZUREDATABRICKS:+defined}"

          # Remplace 836231694088498 par l'ID de ton job "ETL v1"
          response=$(curl -X POST \
            -H "Authorization: Bearer ${{ env.DATABRICKS_TOKEN }}" \
            -H "Content-Type: application/json" \
            "${{ env.DATABRICKS_HOST }}/api/2.1/jobs/run_now" \
            -d '{
              "job_id": 836231694088498,
              "notebook_params": {
                "notebook_path": "/raw_to_bronze_result_and_plv.ipynb"
              }
            }')

          echo "üìã R√©ponse de l'API Databricks :"
          echo "$response"

          # V√©rifier si le lancement a r√©ussi
          run_id=$(echo "$response" | jq -r '.run_id')
          if [ "$run_id" != "null" ]; then
            echo "‚úÖ Pipeline 'ETL v1' lanc√© avec succ√®s (Run ID: $run_id)"
          else
            echo "‚ùå √âchec du lancement du pipeline"
            exit 1
          fi

      - name: üîÑ V√©rification du statut du pipeline
        run: |
          echo "================================"
          echo "üîÑ V√âRIFICATION DU STATUT DU PIPELINE"
          echo "================================"

          # Remplace $run_id par l'ID r√©el de ton run (√† r√©cup√©rer depuis la r√©ponse pr√©c√©dente)
          sleep 30
          status=$(curl -X GET \
            -H "Authorization: Bearer ${{ env.DATABRICKS_TOKEN }}" \
            "${{ env.DATABRICKS_HOST }}/api/2.1/jobs/runs/get?run_id=$run_id" | jq -r '.state.life_cycle_state')

          echo "üìã Statut du pipeline : $status"

          if [ "$status" == "TERMINATED" ]; then
            echo "‚úÖ Pipeline 'ETL v1' termin√© avec succ√®s"
          elif [ "$status" == "RUNNING" ]; then
            echo "‚ÑπÔ∏è  Pipeline en cours d'ex√©cution"
          else
            echo "‚ùå Pipeline en √©chec ou statut inconnu"
            exit 1
          fi

      - name: ‚ùå Gestion des erreurs
        if: failure()
        run: |
          echo "================================"
          echo "‚ùå ERREUR LORS DE L'EX√âCUTION DU PIPELINE"
          echo "================================"

          echo "üìÑ Consultez les logs ci-dessus pour plus de d√©tails"
          echo "üí° V√©rifiez :"
          echo "   - L'ID du job est correct"
          echo "   - Le token Databricks est valide"
          echo "   - Le cluster est disponible"
          exit 1

      - name: ‚úÖ R√©sum√© de l'ex√©cution
        if: success()
        run: |
          echo "================================"
          echo "‚úÖ PIPELINE 'ETL v1' TERMIN√â AVEC SUCC√àS"
          echo "================================"

          echo "üìä R√©sum√© :"
          echo "   ‚úÖ Pipeline 'ETL v1' lanc√© sur Databricks"
          echo "   ‚úÖ Statut v√©rifi√© avec succ√®s"
          echo ""
          echo "üéâ √âtape 3 du pipeline compl√©t√©e !"
