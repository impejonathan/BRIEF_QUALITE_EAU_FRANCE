name: Pipeline QualitÃ© Eau - CI/CD
on:
  workflow_dispatch: {}

env:
  PYTHON_VERSION: '3.10'
  DATABRICKS_HOST: "https://${{ secrets.DATABRICKS_HOST }}"
  DATABRICKS_TOKEN: ${{ secrets.TOKEN_AZUREDATABRICKS }}

jobs:
  # ============================================================
  # JOB 1 : ANALYSE DU CONTENEUR "RAW" (GitHub Actions)
  # ============================================================
  check-raw-files:
    name: "ğŸ” Ã‰tape 1 - Analyse du conteneur 'raw'"
    runs-on: ubuntu-latest
    outputs:
      has_zip: ${{ steps.check-raw.outputs.has_zip }}

    steps:
      - name: ğŸ“¦ Checkout du code
        uses: actions/checkout@v4

      - name: ğŸ Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ğŸ“š Installation des dÃ©pendances Azure
        run: |
          pip install azure-storage-blob

      - name: ğŸ” Configuration des variables d'environnement
        run: |
          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV

      - name: ğŸ” VÃ©rification des fichiers dans le conteneur "raw"
        id: check-raw
        run: |
          echo "================================"
          echo "ğŸ” ANALYSE DU CONTENEUR 'RAW'"
          echo "================================"

          python3 << 'EOF'
          from azure.storage.blob import BlobServiceClient
          import os

          # RÃ©cupÃ©rer les variables d'environnement
          storage_account_name = os.environ.get("AZURE_STORAGE_ACCOUNT_NAME")
          storage_account_key = os.environ.get("AZURE_STORAGE_ACCOUNT_KEY")
          container_name = os.environ.get("CONTAINER_RAW")

          if not all([storage_account_name, storage_account_key, container_name]):
              raise ValueError("âŒ Les variables d'environnement ne sont pas configurÃ©es")

          # Connexion au compte de stockage
          connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net"
          blob_service_client = BlobServiceClient.from_connection_string(connection_string)

          # Lister les fichiers dans le conteneur "raw"
          container_client = blob_service_client.get_container_client(container_name)
          blobs = list(container_client.list_blobs())

          if not blobs:
              print("âŒ Aucun fichier trouvÃ© dans le conteneur 'raw'")
              print("::set-output name=has_zip::false")
          else:
              has_zip = any(blob.name.endswith('.zip') for blob in blobs)
              if has_zip:
                  print("âœ… Fichiers .zip trouvÃ©s dans 'raw' â†’ Passage direct Ã  l'Ã©tape 3")
                  print("::set-output name=has_zip::true")
              else:
                  print("â„¹ï¸  Fichiers trouvÃ©s, mais pas de .zip â†’ ExÃ©cution de l'ingestion locale")
                  print("::set-output name=has_zip::false")
          EOF

  # ============================================================
  # JOB 2 : INGESTION LOCALE (GitHub Actions)
  # ============================================================
  ingestion-local:
    name: "ğŸ“¥ Ã‰tape 2 - Ingestion Locale"
    needs: check-raw-files
    if: needs.check-raw-files.outputs.has_zip == 'false'
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¦ Checkout du code
        uses: actions/checkout@v4

      - name: ğŸ Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ğŸ“š Installation des dÃ©pendances
        run: |
          echo "================================"
          echo "ğŸ“š INSTALLATION DES DÃ‰PENDANCES"
          echo "================================"

          python -m pip install --upgrade pip

          if [ -f requirements.txt ]; then
            echo "ğŸ“„ Installation depuis requirements.txt"
            pip install -r requirements.txt
          fi

          echo "â˜ï¸  Installation des packages Azure"
          pip install azure-storage-blob azure-identity azure-core

          echo "ğŸ““ Installation de papermill"
          pip install papermill nbformat nbconvert jupyter

          pip install python-dotenv

          echo "âœ… Toutes les dÃ©pendances sont installÃ©es"
          pip list | grep azure

      - name: ğŸ” Configuration des variables d'environnement
        run: |
          echo "================================"
          echo "ğŸ” CONFIGURATION DES SECRETS"
          echo "================================"

          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_CONNECTION_STRING=${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV
          echo "CONTAINER_BRONZE=${{ secrets.CONTAINER_BRONZE }}" >> $GITHUB_ENV

          echo "âœ… Variables d'environnement configurÃ©es"

      - name: ğŸ“ VÃ©rification du notebook
        run: |
          echo "================================"
          echo "ğŸ“ VÃ‰RIFICATION DU NOTEBOOK"
          echo "================================"

          if [ ! -f "00_qualite_eau_ingestion.ipynb" ]; then
            echo "âŒ Le fichier 00_qualite_eau_ingestion.ipynb n'existe pas"
            exit 1
          fi

          echo "âœ… Notebook trouvÃ© : 00_qualite_eau_ingestion.ipynb"

          python3 << EOF
          import json
          with open('00_qualite_eau_ingestion.ipynb', 'r', encoding='utf-8') as f:
              nb = json.load(f)
              print(f"ğŸ“Š Nombre de cellules : {len(nb['cells'])}")
              print(f"ğŸ“Š Kernel : {nb['metadata'].get('kernelspec', {}).get('name', 'N/A')}")
          EOF

      - name: ğŸš€ ExÃ©cution du notebook d'ingestion
        run: |
          echo "================================"
          echo "ğŸ“¥ Ã‰TAPE 2 : INGESTION DES DONNÃ‰ES"
          echo "================================"

          export AZURE_STORAGE_ACCOUNT_NAME="${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}"
          export AZURE_STORAGE_ACCOUNT_KEY="${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}"
          export AZURE_STORAGE_CONNECTION_STRING="${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}"
          export CONTAINER_RAW="${{ secrets.CONTAINER_RAW }}"
          export CONTAINER_BRONZE="${{ secrets.CONTAINER_BRONZE }}"

          papermill \
            00_qualite_eau_ingestion.ipynb \
            00_qualite_eau_ingestion_output.ipynb \
            --log-output \
            --progress-bar \
            --request-save-on-cell-execute

          echo ""
          echo "âœ… Ingestion terminÃ©e avec succÃ¨s"

      - name: ğŸ“Š VÃ©rification des rÃ©sultats
        if: success()
        run: |
          echo "================================"
          echo "ğŸ“Š VÃ‰RIFICATION DES RÃ‰SULTATS"
          echo "================================"

          echo "âœ… Notebook exÃ©cutÃ© : 00_qualite_eau_ingestion.ipynb"
          echo "ğŸ“„ Output gÃ©nÃ©rÃ© : 00_qualite_eau_ingestion_output.ipynb"

          if [ -f "00_qualite_eau_ingestion_output.ipynb" ]; then
            echo "âœ… Fichier output crÃ©Ã© avec succÃ¨s"

            python3 << EOF
            import json
            with open('00_qualite_eau_ingestion_output.ipynb', 'r', encoding='utf-8') as f:
                nb = json.load(f)
                cells_executed = sum(1 for cell in nb['cells'] if cell.get('execution_count'))
                print(f"ğŸ“Š Cellules exÃ©cutÃ©es : {cells_executed}/{len(nb['cells'])}")
            EOF
          else
            echo "âš ï¸  Fichier output non trouvÃ©"
          fi

      - name: ğŸ’¾ Upload du notebook exÃ©cutÃ© (artifact)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-ingestion-output
          path: |
            00_qualite_eau_ingestion_output.ipynb
            *.log
          retention-days: 7

      - name: âŒ Gestion des erreurs
        if: failure()
        run: |
          echo "================================"
          echo "âŒ ERREUR LORS DE L'INGESTION"
          echo "================================"

          echo "ğŸ“„ Consultez les logs ci-dessus pour plus de dÃ©tails"
          echo "ğŸ’¡ VÃ©rifiez :"
          echo "   - Les credentials Azure sont corrects"
          echo "   - Le container existe"
          echo "   - Les permissions sont configurÃ©es"
          exit 1

      - name: âœ… RÃ©sumÃ© de l'exÃ©cution
        if: success()
        run: |
          echo "================================"
          echo "âœ… INGESTION TERMINÃ‰E AVEC SUCCÃˆS"
          echo "================================"

          echo "ğŸ“Š RÃ©sumÃ© :"
          echo "   âœ… Notebook exÃ©cutÃ© : 00_qualite_eau_ingestion.ipynb"
          echo "   âœ… Output disponible : 00_qualite_eau_ingestion_output.ipynb"
          echo "   âœ… Artifacts uploadÃ©s : notebook-ingestion-output"
          echo ""
          echo "ğŸ‰ Ã‰tape 2 du pipeline complÃ©tÃ©e !"

  # ============================================================
  # JOB 3 : EXÃ‰CUTION DU PIPELINE "ETL v1" SUR DATABRICKS
  # ============================================================
  run-databricks-pipeline:
    name: "ğŸš€ Ã‰tape 3 - ExÃ©cution du pipeline 'ETL v1' sur Databricks"
    needs: check-raw-files
    if: needs.check-raw-files.outputs.has_zip == 'true' || (needs.check-raw-files.outputs.has_zip == 'false' && needs.ingestion-local.result == 'success')
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¦ Checkout du code
        uses: actions/checkout@v4

      - name: ğŸš€ Lancement du pipeline "ETL v1" sur Databricks
        run: |
          echo "================================"
          echo "ğŸš€ LANCEMENT DU PIPELINE 'ETL v1'"
          echo "================================"

          echo "DATABRICKS_HOST: ${{ env.DATABRICKS_HOST }}"
          echo "DATABRICKS_TOKEN: defined"

          # Remplace 836231694088498 par l'ID de ton job "ETL v1"
          response=$(curl -s -X POST \
            -H "Authorization: Bearer ${{ env.DATABRICKS_TOKEN }}" \
            -H "Content-Type: application/json" \
            "${{ env.DATABRICKS_HOST }}/api/2.1/jobs/run-now" \
            -d '{
              "job_id": 836231694088498,
              "notebook_params": {
                "notebook_path": "/raw_to_bronze_result_and_plv.ipynb"
              }
            }')

          echo "RÃ©ponse complÃ¨te de l'API Databricks :"
          echo "$response"

          # VÃ©rifier si le lancement a rÃ©ussi
          run_id=$(echo "$response" | jq -r '.run_id')
          if [ "$run_id" != "null" ]; then
            echo "âœ… Pipeline 'ETL v1' lancÃ© avec succÃ¨s (Run ID: $run_id)"
            echo "run_id=$run_id" >> $GITHUB_ENV
          else
            echo "âŒ Ã‰chec du lancement du pipeline"
            exit 1
          fi

      - name: ğŸ”„ VÃ©rification du statut du pipeline
        run: |
          echo "================================"
          echo "ğŸ”„ VÃ‰RIFICATION DU STATUT DU PIPELINE"
          echo "================================"

          sleep 30
          response=$(curl -s -X GET \
            -H "Authorization: Bearer ${{ env.DATABRICKS_TOKEN }}" \
            "${{ env.DATABRICKS_HOST }}/api/2.1/jobs/runs/get?run_id=${{ env.run_id }}")

          echo "RÃ©ponse complÃ¨te de l'API Databricks :"
          echo "$response"

          # VÃ©rifier si la rÃ©ponse contient un champ 'state'
          state=$(echo "$response" | jq -r '.state')
          if [ "$state" == "null" ]; then
            echo "âŒ La rÃ©ponse de l'API ne contient pas de champ 'state'"
            exit 1
          fi

          # Extraire le statut du pipeline
          status=$(echo "$response" | jq -r '.state.life_cycle_state')
          result_state=$(echo "$response" | jq -r '.state.result_state')

          echo "ğŸ“‹ Statut du pipeline : $status"
          echo "ğŸ“‹ RÃ©sultat du pipeline : $result_state"

          if [ "$status" == "TERMINATED" ]; then
            if [ "$result_state" == "SUCCESS" ]; then
              echo "âœ… Pipeline 'ETL v1' terminÃ© avec succÃ¨s"
            else
              echo "âŒ Pipeline 'ETL v1' terminÃ© avec Ã©chec (Result State: $result_state)"
              exit 1
            fi
          elif [ "$status" == "RUNNING" ]; then
            echo "â„¹ï¸  Pipeline en cours d'exÃ©cution"
          else
            echo "âŒ Pipeline en Ã©chec ou statut inconnu (Statut: $status)"
            exit 1
          fi

      - name: âŒ Gestion des erreurs
        if: failure()
        run: |
          echo "================================"
          echo "âŒ ERREUR LORS DE L'EXÃ‰CUTION DU PIPELINE"
          echo "================================"

          echo "ğŸ“„ Consultez les logs ci-dessus pour plus de dÃ©tails"
          echo "ğŸ’¡ VÃ©rifiez :"
          echo "   - L'ID du job est correct"
          echo "   - Le token Databricks est valide"
          echo "   - Le cluster est disponible"
          exit 1

      - name: âœ… RÃ©sumÃ© de l'exÃ©cution
        if: success()
        run: |
          echo "================================"
          echo "âœ… PIPELINE 'ETL v1' TERMINÃ‰ AVEC SUCCÃˆS"
          echo "================================"

          echo "ğŸ“Š RÃ©sumÃ© :"
          echo "   âœ… Pipeline 'ETL v1' lancÃ© sur Databricks"
          echo "   âœ… Statut vÃ©rifiÃ© avec succÃ¨s"
          echo ""
          echo "ğŸ‰ Ã‰tape 3 du pipeline complÃ©tÃ©e !"
