name: Pipeline Qualit√© Eau - CI/CD

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  workflow_dispatch:  # Permet de lancer manuellement

env:
  PYTHON_VERSION: '3.10'
  DATABRICKS_HOST: https://adb-<workspace-id>.<region>.azuredatabricks.net  # √Ä MODIFIER
  CLUSTER_NAME: "Jonathan IMPE's Cluster credential"

jobs:
  # ============================================================
  # JOB 1 : INGESTION LOCALE (GitHub Actions)
  # ============================================================
  ingestion-local:
    name: "üì• √âtape 1 - Ingestion Locale"
    runs-on: ubuntu-latest
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üìö Installation des d√©pendances
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install papermill nbformat nbconvert

      - name: üîê Configuration des variables d'environnement
        run: |
          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_CONNECTION_STRING=${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV

      - name: üöÄ Ex√©cution du notebook d'ingestion
        run: |
          echo "================================"
          echo "üì• √âTAPE 1 : INGESTION DES DONN√âES"
          echo "================================"
          
          papermill \
            00_qualite_eau_ingestion.ipynb \
            00_qualite_eau_ingestion_output.ipynb \
            -p storage_account_name "${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" \
            -p storage_account_key "${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" \
            -p container_raw "${{ secrets.CONTAINER_RAW }}" \
            --log-output \
            --progress-bar
          
          echo "‚úÖ Ingestion termin√©e avec succ√®s"

      - name: üìä V√©rification des r√©sultats
        if: success()
        run: |
          echo "‚úÖ Notebook ex√©cut√© : 00_qualite_eau_ingestion.ipynb"
          echo "üìÑ Output : 00_qualite_eau_ingestion_output.ipynb"

      - name: üíæ Upload du notebook ex√©cut√© (artifact)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-ingestion-output
          path: 00_qualite_eau_ingestion_output.ipynb
          retention-days: 7

      - name: ‚ùå Gestion des erreurs
        if: failure()
        run: |
          echo "‚ùå ERREUR lors de l'ingestion"
          echo "üìÑ Consultez les logs ci-dessus"
          exit 1

  # ============================================================
  # JOB 2 : TRANSFORMATION RAW TO BRONZE (Databricks)
  # ============================================================
  raw-to-bronze:
    name: "ü•â √âtape 2 - Raw to Bronze"
    runs-on: ubuntu-latest
    needs: ingestion-local
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üìö Installation Databricks CLI
        run: |
          pip install databricks-cli requests

      - name: üîê Configuration Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
          echo "token = ${{ secrets.TOKEN_AZUREDATABRICKS }}" >> ~/.databrickscfg
          
          echo "‚úÖ Configuration Databricks CLI effectu√©e"

      - name: üîç R√©cup√©ration de l'ID du cluster
        id: get-cluster
        run: |
          echo "üîç Recherche du cluster : ${{ env.CLUSTER_NAME }}"
          
          CLUSTER_ID=$(databricks clusters list --output JSON | \
            python3 -c "import sys, json; clusters = json.load(sys.stdin)['clusters']; print(next((c['cluster_id'] for c in clusters if c['cluster_name'] == '${{ env.CLUSTER_NAME }}'), None))")
          
          if [ -z "$CLUSTER_ID" ] || [ "$CLUSTER_ID" == "None" ]; then
            echo "‚ùå Cluster non trouv√© : ${{ env.CLUSTER_NAME }}"
            exit 1
          fi
          
          echo "‚úÖ Cluster trouv√© : $CLUSTER_ID"
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_OUTPUT
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_ENV

      - name: ‚ñ∂Ô∏è  D√©marrage du cluster (si arr√™t√©)
        run: |
          echo "üîÑ V√©rification de l'√©tat du cluster..."
          
          STATE=$(databricks clusters get --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }} | \
            python3 -c "import sys, json; print(json.load(sys.stdin)['state'])")
          
          echo "√âtat actuel : $STATE"
          
          if [ "$STATE" == "TERMINATED" ]; then
            echo "üöÄ D√©marrage du cluster..."
            databricks clusters start --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }}
            
            # Attendre que le cluster soit pr√™t
            while true; do
              STATE=$(databricks clusters get --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }} | \
                python3 -c "import sys, json; print(json.load(sys.stdin)['state'])")
              
              echo "√âtat : $STATE"
              
              if [ "$STATE" == "RUNNING" ]; then
                echo "‚úÖ Cluster d√©marr√©"
                break
              elif [ "$STATE" == "ERROR" ]; then
                echo "‚ùå Erreur de d√©marrage du cluster"
                exit 1
              fi
              
              sleep 30
            done
          else
            echo "‚úÖ Cluster d√©j√† actif"
          fi

      - name: üì§ Upload du notebook vers Databricks
        run: |
          echo "üì§ Upload de raw_to_bronze_result_and_plv.ipynb"
          
          databricks workspace import \
            raw_to_bronze_result_and_plv.ipynb \
            /Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/raw_to_bronze_result_and_plv \
            --language PYTHON \
            --format JUPYTER \
            --overwrite
          
          echo "‚úÖ Notebook upload√©"

      - name: üöÄ Ex√©cution du notebook sur Databricks
        id: run-notebook-bronze
        run: |
          echo "================================"
          echo "ü•â √âTAPE 2 : RAW TO BRONZE"
          echo "================================"
          
          # Cr√©er un job run
          RUN_ID=$(databricks runs submit --json '{
            "run_name": "Pipeline - Raw to Bronze",
            "existing_cluster_id": "${{ steps.get-cluster.outputs.CLUSTER_ID }}",
            "notebook_task": {
              "notebook_path": "/Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/raw_to_bronze_result_and_plv",
              "base_parameters": {}
            },
            "timeout_seconds": 3600
          }' | python3 -c "import sys, json; print(json.load(sys.stdin)['run_id'])")
          
          echo "‚úÖ Job lanc√© : Run ID = $RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_OUTPUT
          
          # Attendre la fin du job
          echo "‚è≥ Attente de la fin du job..."
          
          while true; do
            STATUS=$(databricks runs get --run-id $RUN_ID | \
              python3 -c "import sys, json; print(json.load(sys.stdin)['state']['life_cycle_state'])")
            
            echo "Statut : $STATUS"
            
            if [ "$STATUS" == "TERMINATED" ]; then
              RESULT=$(databricks runs get --run-id $RUN_ID | \
                python3 -c "import sys, json; print(json.load(sys.stdin)['state']['result_state'])")
              
              if [ "$RESULT" == "SUCCESS" ]; then
                echo "‚úÖ Job termin√© avec succ√®s"
                break
              else
                echo "‚ùå Job √©chou√© : $RESULT"
                exit 1
              fi
            elif [ "$STATUS" == "SKIPPED" ] || [ "$STATUS" == "INTERNAL_ERROR" ]; then
              echo "‚ùå Erreur : $STATUS"
              exit 1
            fi
            
            sleep 20
          done

      - name: üìã R√©cup√©ration des logs
        if: always()
        run: |
          echo "üìã Logs du job ${{ steps.run-notebook-bronze.outputs.RUN_ID }}"
          databricks runs get-output --run-id ${{ steps.run-notebook-bronze.outputs.RUN_ID }} || true

  # ============================================================
  # JOB 3 : CR√âATION DES TABLES (Databricks)
  # ============================================================
  create-tables:
    name: "üìä √âtape 3 - Cr√©ation Tables"
    runs-on: ubuntu-latest
    needs: raw-to-bronze
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üìö Installation Databricks CLI
        run: |
          pip install databricks-cli requests

      - name: üîê Configuration Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
          echo "token = ${{ secrets.TOKEN_AZUREDATABRICKS }}" >> ~/.databrickscfg

      - name: üîç R√©cup√©ration de l'ID du cluster
        id: get-cluster
        run: |
          CLUSTER_ID=$(databricks clusters list --output JSON | \
            python3 -c "import sys, json; clusters = json.load(sys.stdin)['clusters']; print(next((c['cluster_id'] for c in clusters if c['cluster_name'] == '${{ env.CLUSTER_NAME }}'), None))")
          
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_OUTPUT
          echo "‚úÖ Cluster : $CLUSTER_ID"

      - name: üì§ Upload du notebook
        run: |
          echo "üì§ Upload de read_create_table.ipynb"
          
          databricks workspace import \
            read_create_table.ipynb \
            /Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/read_create_table \
            --language PYTHON \
            --format JUPYTER \
            --overwrite
          
          echo "‚úÖ Notebook upload√©"

      - name: üöÄ Ex√©cution du notebook
        id: run-notebook-tables
        run: |
          echo "================================"
          echo "üìä √âTAPE 3 : CR√âATION TABLES"
          echo "================================"
          
          RUN_ID=$(databricks runs submit --json '{
            "run_name": "Pipeline - Create Tables",
            "existing_cluster_id": "${{ steps.get-cluster.outputs.CLUSTER_ID }}",
            "notebook_task": {
              "notebook_path": "/Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/read_create_table",
              "base_parameters": {}
            },
            "timeout_seconds": 3600
          }' | python3 -c "import sys, json; print(json.load(sys.stdin)['run_id'])")
          
          echo "‚úÖ Job lanc√© : Run ID = $RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_OUTPUT
          
          # Attendre la fin
          while true; do
            STATUS=$(databricks runs get --run-id $RUN_ID | \
              python3 -c "import sys, json; print(json.load(sys.stdin)['state']['life_cycle_state'])")
            
            if [ "$STATUS" == "TERMINATED" ]; then
              RESULT=$(databricks runs get --run-id $RUN_ID | \
                python3 -c "import sys, json; print(json.load(sys.stdin)['state']['result_state'])")
              
              if [ "$RESULT" == "SUCCESS" ]; then
                echo "‚úÖ Tables cr√©√©es avec succ√®s"
                break
              else
                echo "‚ùå √âchec : $RESULT"
                exit 1
              fi
            fi
            sleep 20
          done

  # ============================================================
  # JOB 4 : TRANSFORMATION SILVER (Databricks)
  # ============================================================
  silver-transformation:
    name: "ü•à √âtape 4 - Silver Transformation"
    runs-on: ubuntu-latest
    needs: create-tables
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üìö Installation Databricks CLI
        run: |
          pip install databricks-cli requests

      - name: üîê Configuration Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
          echo "token = ${{ secrets.TOKEN_AZUREDATABRICKS }}" >> ~/.databrickscfg

      - name: üîç R√©cup√©ration de l'ID du cluster
        id: get-cluster
        run: |
          CLUSTER_ID=$(databricks clusters list --output JSON | \
            python3 -c "import sys, json; clusters = json.load(sys.stdin)['clusters']; print(next((c['cluster_id'] for c in clusters if c['cluster_name'] == '${{ env.CLUSTER_NAME }}'), None))")
          
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_OUTPUT
          echo "‚úÖ Cluster : $CLUSTER_ID"

      - name: üì§ Upload du notebook
        run: |
          echo "üì§ Upload de 03_Silver_Transformation_azuredatabricks.ipynb"
          
          databricks workspace import \
            03_Silver_Transformation_azuredatabricks.ipynb \
            /Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/03_Silver_Transformation_azuredatabricks \
            --language PYTHON \
            --format JUPYTER \
            --overwrite
          
          echo "‚úÖ Notebook upload√©"

      - name: üöÄ Ex√©cution du notebook
        id: run-notebook-silver
        run: |
          echo "================================"
          echo "ü•à √âTAPE 4 : SILVER TRANSFORMATION"
          echo "================================"
          
          RUN_ID=$(databricks runs submit --json '{
            "run_name": "Pipeline - Silver Transformation",
            "existing_cluster_id": "${{ steps.get-cluster.outputs.CLUSTER_ID }}",
            "notebook_task": {
              "notebook_path": "/Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/03_Silver_Transformation_azuredatabricks",
              "base_parameters": {}
            },
            "timeout_seconds": 3600
          }' | python3 -c "import sys, json; print(json.load(sys.stdin)['run_id'])")
          
          echo "‚úÖ Job lanc√© : Run ID = $RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_OUTPUT
          
          # Attendre la fin
          while true; do
            STATUS=$(databricks runs get --run-id $RUN_ID | \
              python3 -c "import sys, json; print(json.load(sys.stdin)['state']['life_cycle_state'])")
            
            if [ "$STATUS" == "TERMINATED" ]; then
              RESULT=$(databricks runs get --run-id $RUN_ID | \
                python3 -c "import sys, json; print(json.load(sys.stdin)['state']['result_state'])")
              
              if [ "$RESULT" == "SUCCESS" ]; then
                echo "‚úÖ Transformation Silver termin√©e avec succ√®s"
                break
              else
                echo "‚ùå √âchec : $RESULT"
                exit 1
              fi
            fi
            sleep 20
          done

      - name: üìã R√©cup√©ration des logs
        if: always()
        run: |
          databricks runs get-output --run-id ${{ steps.run-notebook-silver.outputs.RUN_ID }} || true

  # ============================================================
  # JOB 5 : R√âSUM√â FINAL
  # ============================================================
  summary:
    name: "üìä R√©sum√© du Pipeline"
    runs-on: ubuntu-latest
    needs: [ingestion-local, raw-to-bronze, create-tables, silver-transformation]
    if: always()
    
    steps:
      - name: üìä R√©sum√© de l'ex√©cution
        run: |
          echo "================================"
          echo "üìä R√âSUM√â DU PIPELINE"
          echo "================================"
          echo ""
          echo "‚úÖ √âtape 1 : Ingestion locale"
          echo "‚úÖ √âtape 2 : Raw to Bronze"
          echo "‚úÖ √âtape 3 : Cr√©ation des tables"
          echo "‚úÖ √âtape 4 : Silver Transformation"
          echo ""
          echo "================================"
          echo "üéâ PIPELINE TERMIN√â AVEC SUCC√àS"
          echo "================================"