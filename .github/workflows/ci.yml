name: Pipeline Qualit√© Eau - CI/CD

on:
  workflow_dispatch: {}
#   push:
#     branches:
#       - main
#       - develop
#   pull_request:
#     branches:
#       - main

env:
  PYTHON_VERSION: '3.10'
  DATABRICKS_HOST: 'https://adb-<workspace-id>.<region>.azuredatabricks.net'  # ‚ö†Ô∏è √Ä MODIFIER avec votre URL
  CLUSTER_NAME: "Jonathan IMPE's Cluster credential"

jobs:
  # ============================================================
  # JOB 1 : INGESTION LOCALE (GitHub Actions)
  # ============================================================
  ingestion-local:
    name: "üì• √âtape 1 - Ingestion Locale"
    runs-on: ubuntu-latest
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üìö Installation des d√©pendances
        run: |
          echo "================================"
          echo "üìö INSTALLATION DES D√âPENDANCES"
          echo "================================"
          
          python -m pip install --upgrade pip
          
          # Installation des d√©pendances du projet
          if [ -f requirements.txt ]; then
            echo "üìÑ Installation depuis requirements.txt"
            pip install -r requirements.txt
          fi
          
          # Installation des d√©pendances Azure (obligatoires)
          echo "‚òÅÔ∏è  Installation des packages Azure"
          pip install azure-storage-blob azure-identity azure-core
          
          # Installation de papermill pour ex√©cuter les notebooks
          echo "üìì Installation de papermill"
          pip install papermill nbformat nbconvert jupyter
          
          # Installation de python-dotenv si n√©cessaire
          pip install python-dotenv
          
          echo "‚úÖ Toutes les d√©pendances sont install√©es"
          
          # Afficher les packages install√©s
          echo ""
          echo "üìã Packages Azure install√©s :"
          pip list | grep azure

      - name: üîê Configuration des variables d'environnement
        run: |
          echo "================================"
          echo "üîê CONFIGURATION DES SECRETS"
          echo "================================"
          
          echo "AZURE_STORAGE_ACCOUNT_NAME=${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_ACCOUNT_KEY=${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}" >> $GITHUB_ENV
          echo "AZURE_STORAGE_CONNECTION_STRING=${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" >> $GITHUB_ENV
          echo "CONTAINER_RAW=${{ secrets.CONTAINER_RAW }}" >> $GITHUB_ENV
          echo "CONTAINER_BRONZE=${{ secrets.CONTAINER_BRONZE }}" >> $GITHUB_ENV
          
          echo "‚úÖ Variables d'environnement configur√©es"
          echo "üìä Storage Account: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}"
          echo "üìä Container Raw: ${{ secrets.CONTAINER_RAW }}"

      - name: üìù V√©rification du notebook
        run: |
          echo "================================"
          echo "üìù V√âRIFICATION DU NOTEBOOK"
          echo "================================"
          
          if [ ! -f "00_qualite_eau_ingestion.ipynb" ]; then
            echo "‚ùå Le fichier 00_qualite_eau_ingestion.ipynb n'existe pas"
            exit 1
          fi
          
          echo "‚úÖ Notebook trouv√© : 00_qualite_eau_ingestion.ipynb"
          
          # Afficher la structure du notebook
          python3 << EOF
          import json
          with open('00_qualite_eau_ingestion.ipynb', 'r', encoding='utf-8') as f:
              nb = json.load(f)
              print(f"üìä Nombre de cellules : {len(nb['cells'])}")
              print(f"üìä Kernel : {nb['metadata'].get('kernelspec', {}).get('name', 'N/A')}")
          EOF

      - name: üöÄ Ex√©cution du notebook d'ingestion
        run: |
          echo "================================"
          echo "üì• √âTAPE 1 : INGESTION DES DONN√âES"
          echo "================================"
          
          # Exporter les variables pour le notebook
          export AZURE_STORAGE_ACCOUNT_NAME="${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}"
          export AZURE_STORAGE_ACCOUNT_KEY="${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}"
          export AZURE_STORAGE_CONNECTION_STRING="${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}"
          export CONTAINER_RAW="${{ secrets.CONTAINER_RAW }}"
          export CONTAINER_BRONZE="${{ secrets.CONTAINER_BRONZE }}"
          
          # Ex√©cuter le notebook avec papermill (SANS param√®tres)
          papermill \
            00_qualite_eau_ingestion.ipynb \
            00_qualite_eau_ingestion_output.ipynb \
            --log-output \
            --progress-bar \
            --request-save-on-cell-execute
          
          echo ""
          echo "‚úÖ Ingestion termin√©e avec succ√®s"

      - name: üìä V√©rification des r√©sultats
        if: success()
        run: |
          echo "================================"
          echo "üìä V√âRIFICATION DES R√âSULTATS"
          echo "================================"
          
          echo "‚úÖ Notebook ex√©cut√© : 00_qualite_eau_ingestion.ipynb"
          echo "üìÑ Output g√©n√©r√© : 00_qualite_eau_ingestion_output.ipynb"
          
          # V√©rifier que le fichier output existe
          if [ -f "00_qualite_eau_ingestion_output.ipynb" ]; then
            echo "‚úÖ Fichier output cr√©√© avec succ√®s"
            
            # Afficher un r√©sum√© du notebook ex√©cut√©
            python3 << EOF
          import json
          with open('00_qualite_eau_ingestion_output.ipynb', 'r', encoding='utf-8') as f:
              nb = json.load(f)
              cells_executed = sum(1 for cell in nb['cells'] if cell.get('execution_count'))
              print(f"üìä Cellules ex√©cut√©es : {cells_executed}/{len(nb['cells'])}")
          EOF
          else
            echo "‚ö†Ô∏è  Fichier output non trouv√©"
          fi

      - name: üíæ Upload du notebook ex√©cut√© (artifact)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-ingestion-output
          path: |
            00_qualite_eau_ingestion_output.ipynb
            *.log
          retention-days: 7

      - name: üìã Affichage des logs en cas d'erreur
        if: failure()
        run: |
          echo "================================"
          echo "üìã LOGS D'ERREUR"
          echo "================================"
          
          if [ -f "00_qualite_eau_ingestion_output.ipynb" ]; then
            echo "üìÑ Extraction des erreurs du notebook..."
            
            python3 << EOF
          import json
          with open('00_qualite_eau_ingestion_output.ipynb', 'r', encoding='utf-8') as f:
              nb = json.load(f)
              for i, cell in enumerate(nb['cells'], 1):
                  if cell.get('cell_type') == 'code':
                      outputs = cell.get('outputs', [])
                      for output in outputs:
                          if output.get('output_type') == 'error':
                              print(f"\n‚ùå ERREUR dans la cellule {i}:")
                              print(f"   Type: {output.get('ename', 'N/A')}")
                              print(f"   Message: {output.get('evalue', 'N/A')}")
                              traceback = output.get('traceback', [])
                              if traceback:
                                  print("   Traceback:")
                                  for line in traceback:
                                      print(f"   {line}")
          EOF
          fi

      - name: ‚ùå Gestion des erreurs
        if: failure()
        run: |
          echo ""
          echo "================================"
          echo "‚ùå ERREUR LORS DE L'INGESTION"
          echo "================================"
          echo ""
          echo "üìÑ Consultez les logs ci-dessus pour plus de d√©tails"
          echo "üí° V√©rifiez :"
          echo "   - Les credentials Azure sont corrects"
          echo "   - Le container existe"
          echo "   - Les permissions sont configur√©es"
          echo ""
          exit 1

      - name: ‚úÖ R√©sum√© de l'ex√©cution
        if: success()
        run: |
          echo ""
          echo "================================"
          echo "‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS"
          echo "================================"
          echo ""
          echo "üìä R√©sum√© :"
          echo "   ‚úÖ Notebook ex√©cut√© : 00_qualite_eau_ingestion.ipynb"
          echo "   ‚úÖ Output disponible : 00_qualite_eau_ingestion_output.ipynb"
          echo "   ‚úÖ Artifacts upload√©s : notebook-ingestion-output"
          echo ""
          echo "üéâ √âtape 1 du pipeline compl√©t√©e !"
          echo "================================"

  # ============================================================
  # JOB 2 : TRANSFORMATION RAW TO BRONZE (Databricks)
  # ============================================================
  raw-to-bronze:
    name: "ü•â √âtape 2 - Raw to Bronze (Databricks)"
    runs-on: ubuntu-latest
    needs: ingestion-local
    
    steps:
      - name: üì¶ Checkout du code
        uses: actions/checkout@v4

      - name: üêç Configuration Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üìö Installation Databricks CLI
        run: |
          echo "================================"
          echo "üìö INSTALLATION DATABRICKS CLI"
          echo "================================"
          
          pip install databricks-cli requests
          
          echo "‚úÖ Databricks CLI install√©"
          databricks --version

      - name: üîê Configuration Databricks
        run: |
          echo "================================"
          echo "üîê CONFIGURATION DATABRICKS"
          echo "================================"
          
          # Cr√©er le fichier de configuration
          mkdir -p ~/.databricks
          cat > ~/.databrickscfg << EOF
          [DEFAULT]
          host = ${{ env.DATABRICKS_HOST }}
          token = ${{ secrets.TOKEN_AZUREDATABRICKS }}
          EOF
          
          echo "‚úÖ Configuration Databricks effectu√©e"
          echo "üåê Host: ${{ env.DATABRICKS_HOST }}"
          
          # Tester la connexion
          echo ""
          echo "üß™ Test de connexion..."
          databricks clusters list > /dev/null 2>&1
          if [ $? -eq 0 ]; then
            echo "‚úÖ Connexion √† Databricks r√©ussie"
          else
            echo "‚ùå √âchec de connexion √† Databricks"
            exit 1
          fi

      - name: üîç Recherche du cluster
        id: get-cluster
        run: |
          echo "================================"
          echo "üîç RECHERCHE DU CLUSTER"
          echo "================================"
          
          echo "üîç Recherche du cluster : ${{ env.CLUSTER_NAME }}"
          
          # R√©cup√©rer l'ID du cluster
          CLUSTER_ID=$(databricks clusters list --output JSON 2>/dev/null | \
            python3 -c "
          import sys, json
          try:
              data = json.load(sys.stdin)
              clusters = data.get('clusters', [])
              target = '${{ env.CLUSTER_NAME }}'
              for c in clusters:
                  if c.get('cluster_name') == target:
                      print(c['cluster_id'])
                      sys.exit(0)
              print('NOT_FOUND')
          except Exception as e:
              print(f'ERROR: {e}', file=sys.stderr)
              print('ERROR')
          ")
          
          echo "R√©sultat de la recherche : $CLUSTER_ID"
          
          if [ "$CLUSTER_ID" == "NOT_FOUND" ] || [ "$CLUSTER_ID" == "ERROR" ] || [ -z "$CLUSTER_ID" ]; then
            echo "‚ùå Cluster non trouv√© : ${{ env.CLUSTER_NAME }}"
            echo ""
            echo "üìã Clusters disponibles :"
            databricks clusters list
            exit 1
          fi
          
          echo "‚úÖ Cluster trouv√© : $CLUSTER_ID"
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_OUTPUT
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_ENV

      - name: üîÑ V√©rification et d√©marrage du cluster
        run: |
          echo "================================"
          echo "üîÑ GESTION DU CLUSTER"
          echo "================================"
          
          echo "üîç V√©rification de l'√©tat du cluster ${{ steps.get-cluster.outputs.CLUSTER_ID }}..."
          
          # R√©cup√©rer l'√©tat du cluster
          STATE=$(databricks clusters get --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }} 2>/dev/null | \
            python3 -c "import sys, json; print(json.load(sys.stdin)['state'])" 2>/dev/null || echo "UNKNOWN")
          
          echo "üìä √âtat actuel du cluster : $STATE"
          
          if [ "$STATE" == "TERMINATED" ] || [ "$STATE" == "TERMINATING" ]; then
            echo "üöÄ D√©marrage du cluster en cours..."
            databricks clusters start --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }}
            
            echo "‚è≥ Attente du d√©marrage du cluster..."
            TIMEOUT=600  # 10 minutes
            ELAPSED=0
            
            while [ $ELAPSED -lt $TIMEOUT ]; do
              sleep 15
              ELAPSED=$((ELAPSED + 15))
              
              STATE=$(databricks clusters get --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }} 2>/dev/null | \
                python3 -c "import sys, json; print(json.load(sys.stdin)['state'])" 2>/dev/null || echo "UNKNOWN")
              
              echo "‚è±Ô∏è  [$ELAPSED s] √âtat : $STATE"
              
              if [ "$STATE" == "RUNNING" ]; then
                echo "‚úÖ Cluster d√©marr√© et pr√™t"
                break
              elif [ "$STATE" == "ERROR" ]; then
                echo "‚ùå Erreur lors du d√©marrage du cluster"
                exit 1
              fi
            done
            
            if [ $ELAPSED -ge $TIMEOUT ]; then
              echo "‚ùå Timeout : le cluster n'a pas d√©marr√© dans les temps"
              exit 1
            fi
            
          elif [ "$STATE" == "RUNNING" ]; then
            echo "‚úÖ Cluster d√©j√† en cours d'ex√©cution"
          elif [ "$STATE" == "PENDING" ] || [ "$STATE" == "RESTARTING" ]; then
            echo "‚è≥ Cluster en cours de d√©marrage, attente..."
            sleep 30
            
            STATE=$(databricks clusters get --cluster-id ${{ steps.get-cluster.outputs.CLUSTER_ID }} 2>/dev/null | \
              python3 -c "import sys, json; print(json.load(sys.stdin)['state'])" 2>/dev/null)
            
            if [ "$STATE" == "RUNNING" ]; then
              echo "‚úÖ Cluster pr√™t"
            else
              echo "‚ö†Ô∏è  Cluster toujours en d√©marrage (√©tat: $STATE)"
            fi
          else
            echo "‚ö†Ô∏è  √âtat du cluster inconnu ou probl√©matique : $STATE"
          fi

      - name: üì§ Upload du notebook vers Databricks
        run: |
          echo "================================"
          echo "üì§ UPLOAD DU NOTEBOOK"
          echo "================================"
          
          NOTEBOOK_PATH="/Workspace/Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/raw_to_bronze_result_and_plv"
          
          echo "üìÑ Notebook local : raw_to_bronze_result_and_plv.ipynb"
          echo "üìç Destination : $NOTEBOOK_PATH"
          
          # V√©rifier que le notebook existe
          if [ ! -f "raw_to_bronze_result_and_plv.ipynb" ]; then
            echo "‚ùå Le fichier raw_to_bronze_result_and_plv.ipynb n'existe pas"
            exit 1
          fi
          
          # Upload du notebook
          databricks workspace import \
            raw_to_bronze_result_and_plv.ipynb \
            "$NOTEBOOK_PATH" \
            --language PYTHON \
            --format JUPYTER \
            --overwrite
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Notebook upload√© avec succ√®s"
          else
            echo "‚ùå √âchec de l'upload du notebook"
            exit 1
          fi

      - name: üöÄ Ex√©cution du notebook sur Databricks
        id: run-notebook
        run: |
          echo "================================"
          echo "üöÄ EX√âCUTION DU NOTEBOOK"
          echo "================================"
          
          NOTEBOOK_PATH="/Workspace/Users/${{ secrets.DATABRICKS_USER_EMAIL }}/pipeline/raw_to_bronze_result_and_plv"
          
          echo "üìì Notebook : $NOTEBOOK_PATH"
          echo "üñ•Ô∏è  Cluster : ${{ steps.get-cluster.outputs.CLUSTER_ID }}"
          
          # Cr√©er et lancer le job
          RUN_OUTPUT=$(databricks runs submit --json '{
            "run_name": "Pipeline CI/CD - Raw to Bronze",
            "existing_cluster_id": "${{ steps.get-cluster.outputs.CLUSTER_ID }}",
            "notebook_task": {
              "notebook_path": "'"$NOTEBOOK_PATH"'",
              "base_parameters": {}
            },
            "timeout_seconds": 3600,
            "libraries": []
          }' 2>&1)
          
          echo "üìã R√©ponse de l'API :"
          echo "$RUN_OUTPUT"
          
          # Extraire le run_id
          RUN_ID=$(echo "$RUN_OUTPUT" | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('run_id', ''))" 2>/dev/null)
          
          if [ -z "$RUN_ID" ]; then
            echo "‚ùå Impossible de r√©cup√©rer le run_id"
            echo "$RUN_OUTPUT"
            exit 1
          fi
          
          echo "‚úÖ Job lanc√© avec succ√®s"
          echo "üÜî Run ID : $RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_OUTPUT
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV
          
          # Afficher le lien vers le job
          echo "üîó Lien : ${{ env.DATABRICKS_HOST }}/#job/runs/$RUN_ID"

      - name: ‚è≥ Attente de la fin du job
        run: |
          echo "================================"
          echo "‚è≥ SUIVI DE L'EX√âCUTION"
          echo "================================"
          
          RUN_ID="${{ steps.run-notebook.outputs.RUN_ID }}"
          echo "üÜî Suivi du run : $RUN_ID"
          echo ""
          
          TIMEOUT=1800  # 30 minutes
          ELAPSED=0
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            sleep 20
            ELAPSED=$((ELAPSED + 20))
            
            # R√©cup√©rer l'√©tat du job
            JOB_INFO=$(databricks runs get --run-id $RUN_ID 2>/dev/null)
            
            LIFE_CYCLE_STATE=$(echo "$JOB_INFO" | python3 -c "import sys, json; print(json.load(sys.stdin)['state']['life_cycle_state'])" 2>/dev/null || echo "UNKNOWN")
            
            echo "‚è±Ô∏è  [$ELAPSED s] √âtat : $LIFE_CYCLE_STATE"
            
            if [ "$LIFE_CYCLE_STATE" == "TERMINATED" ]; then
              RESULT_STATE=$(echo "$JOB_INFO" | python3 -c "import sys, json; print(json.load(sys.stdin)['state']['result_state'])" 2>/dev/null)
              
              echo ""
              echo "üèÅ Job termin√©"
              echo "üìä R√©sultat : $RESULT_STATE"
              
              if [ "$RESULT_STATE" == "SUCCESS" ]; then
                echo "‚úÖ Job ex√©cut√© avec succ√®s"
                exit 0
              else
                echo "‚ùå Job √©chou√© : $RESULT_STATE"
                
                # Afficher le message d'erreur
                STATE_MESSAGE=$(echo "$JOB_INFO" | python3 -c "import sys, json; print(json.load(sys.stdin)['state'].get('state_message', 'Aucun message'))" 2>/dev/null)
                echo "üìÑ Message : $STATE_MESSAGE"
                
                exit 1
              fi
            elif [ "$LIFE_CYCLE_STATE" == "SKIPPED" ] || [ "$LIFE_CYCLE_STATE" == "INTERNAL_ERROR" ]; then
              echo "‚ùå Erreur : $LIFE_CYCLE_STATE"
              exit 1
            fi
          done
          
          if [ $ELAPSED -ge $TIMEOUT ]; then
            echo "‚ùå Timeout : le job n'a pas termin√© dans les temps"
            exit 1
          fi

      - name: üìã R√©cup√©ration des logs
        if: always()
        run: |
          echo "================================"
          echo "üìã LOGS DU JOB"
          echo "================================"
          
          RUN_ID="${{ steps.run-notebook.outputs.RUN_ID }}"
          
          if [ ! -z "$RUN_ID" ]; then
            echo "üÜî Run ID : $RUN_ID"
            databricks runs get-output --run-id $RUN_ID 2>&1 || echo "‚ö†Ô∏è  Impossible de r√©cup√©rer les logs"
          else
            echo "‚ö†Ô∏è  Aucun run_id disponible"
          fi

      - name: ‚úÖ R√©sum√© de l'√©tape 2
        if: success()
        run: |
          echo ""
          echo "================================"
          echo "‚úÖ RAW TO BRONZE TERMIN√â"
          echo "================================"
          echo ""
          echo "üìä R√©sum√© :"
          echo "   ‚úÖ Notebook ex√©cut√© : raw_to_bronze_result_and_plv.ipynb"
          echo "   ‚úÖ Cluster : ${{ env.CLUSTER_NAME }}"
          echo "   ‚úÖ Run ID : ${{ steps.run-notebook.outputs.RUN_ID }}"
          echo ""
          echo "üéâ √âtape 2 du pipeline compl√©t√©e !"
          echo "================================"

      - name: ‚ùå Gestion des erreurs
        if: failure()
        run: |
          echo ""
          echo "================================"
          echo "‚ùå ERREUR √âTAPE 2 : RAW TO BRONZE"
          echo "================================"
          echo ""
          echo "üí° V√©rifiez :"
          echo "   - Le cluster est accessible"
          echo "   - Le token Databricks est valide"
          echo "   - Le notebook existe et est valide"
          echo "   - Les logs ci-dessus pour plus de d√©tails"
          echo ""
          exit 1