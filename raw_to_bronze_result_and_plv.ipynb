{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f71f14-e52b-42ab-862e-8b85bf1a34df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ## 1. Configuration et connexion au Storage Azure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29b8a4c-3a35-4dde-a567-bf8e9bc844a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pyspark.sql.functions import lit, col, count, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß CONFIGURATION DATABRICKS ‚Üí AZURE STORAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# R√©cup√©rer les variables d'environnement configur√©es dans le cluster\n",
    "storage_account_name = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "\n",
    "# V√©rification\n",
    "if not storage_account_name or not storage_account_key:\n",
    "    raise ValueError(\"‚ùå Les variables d'environnement ne sont pas configur√©es dans le cluster\")\n",
    "\n",
    "print(\"‚úÖ Variables d'environnement r√©cup√©r√©es avec succ√®s\")\n",
    "print(f\"üì¶ Compte de stockage : {storage_account_name}\")\n",
    "\n",
    "# Configuration de la connexion Spark\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration Spark effectu√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6708a932-4366-4799-bded-84aed2c23502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ## 2. Montage des conteneurs RAW et BRONZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3972076-f2c5-489c-be71-6222a0459be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def mount_container(container_name, mount_point):\n",
    "    \"\"\"\n",
    "    Monte un conteneur Blob Storage dans le syst√®me de fichiers Databricks\n",
    "    G√®re automatiquement ADLS Gen2 et Blob Storage classique\n",
    "    \n",
    "    Args:\n",
    "        container_name (str): Nom du conteneur (raw, bronze, etc.)\n",
    "        mount_point (str): Point de montage dans DBFS (ex: /mnt/raw)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # V√©rifier si d√©j√† mont√©\n",
    "        if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "            print(f\"‚ö†Ô∏è  {mount_point} est d√©j√† mont√©. D√©montage...\")\n",
    "            dbutils.fs.unmount(mount_point)\n",
    "        \n",
    "        # M√©thode 1 : WASBS (Blob Storage Gen V2 sans Hierarchical Namespace)\n",
    "        print(f\"üîÑ Tentative de montage avec WASBS pour '{container_name}'...\")\n",
    "        \n",
    "        configs = {\n",
    "            f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key\n",
    "        }\n",
    "        \n",
    "        source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source=source,\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Conteneur '{container_name}' mont√© sur '{mount_point}' (WASBS)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  √âchec WASBS : {str(e)}\")\n",
    "        \n",
    "        # M√©thode 2 : ABFSS (ADLS Gen2 avec Hierarchical Namespace)\n",
    "        try:\n",
    "            print(f\"üîÑ Tentative de montage avec ABFSS pour '{container_name}'...\")\n",
    "            \n",
    "            # D√©monter si la premi√®re tentative a partiellement r√©ussi\n",
    "            try:\n",
    "                dbutils.fs.unmount(mount_point)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            configs = {\n",
    "                f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\": storage_account_key\n",
    "            }\n",
    "            \n",
    "            source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "            \n",
    "            dbutils.fs.mount(\n",
    "                source=source,\n",
    "                mount_point=mount_point,\n",
    "                extra_configs=configs\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Conteneur '{container_name}' mont√© sur '{mount_point}' (ABFSS)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå √âchec ABFSS : {str(e2)}\")\n",
    "            print(f\"‚ùå Impossible de monter '{container_name}'\")\n",
    "            return False\n",
    "\n",
    "# Monter les conteneurs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó MONTAGE DES CONTENEURS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mount_container(\"raw\", \"/mnt/raw\")\n",
    "print()\n",
    "mount_container(\"bronze\", \"/mnt/bronze\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "################################################################################################################################################\n",
    "################################################################################################################################################\n",
    "# %% [markdown]\n",
    "# ## 3. V√©rification des connexions ########################################################################\n",
    "################################################################################################################################################\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÇ V√âRIFICATION DES MONTAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Lister tous les montages\n",
    "for mount in dbutils.fs.mounts():\n",
    "    print(f\"üìç {mount.mountPoint} -> {mount.source}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÇ CONTENU DU CONTENEUR RAW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/mnt/raw\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"üì≠ Le conteneur RAW est vide\")\n",
    "    else:\n",
    "        for idx, file in enumerate(files, 1):\n",
    "            size_mb = file.size / (1024 * 1024)\n",
    "            print(f\"\\n{idx}. {file.name}\")\n",
    "            print(f\"   üìè Taille : {size_mb:.2f} MB\")\n",
    "            print(f\"   üìç Chemin : {file.path}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÇ CONTENU DU CONTENEUR BRONZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"üì≠ Le conteneur BRONZE est vide\")\n",
    "    else:\n",
    "        for idx, file in enumerate(files, 1):\n",
    "            size_mb = file.size / (1024 * 1024)\n",
    "            print(f\"\\n{idx}. {file.name}\")\n",
    "            print(f\"   üìè Taille : {size_mb:.2f} MB\")\n",
    "            print(f\"   üìç Chemin : {file.path}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42053a7-89c5-4499-9417-dafc81f87fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Extraction des fichiers ZIP (DIS_PLV + DIS_RESULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2628e68f-1517-4118-bfe8-2701aaf4fdcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Extraction des fichiers ZIP (DIS_PLV + DIS_RESULT)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PIPELINE D'EXTRACTION RAW ‚Üí BRONZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.1. Fonction d'extraction DIS_PLV + DIS_RESULT\n",
    "\n",
    "# %%\n",
    "def extract_dis_files_from_zip(zip_path, destination_container=\"/mnt/bronze\"):\n",
    "    \"\"\"\n",
    "    Extrait les fichiers DIS_PLV_*.txt et DIS_RESULT_*.txt d'un ZIP et les charge dans BRONZE\n",
    "    \n",
    "    Args:\n",
    "        zip_path (str): Chemin du fichier ZIP dans RAW\n",
    "        destination_container (str): Chemin du conteneur de destination\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistiques de traitement\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'zip_name': zip_path.split('/')[-1],\n",
    "        'total_files': 0,\n",
    "        'dis_plv_files': 0,\n",
    "        'dis_result_files': 0,\n",
    "        'extracted_plv': [],\n",
    "        'extracted_result': [],\n",
    "        'skipped_files': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üì¶ Traitement : {stats['zip_name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Nettoyer le chemin\n",
    "        clean_path = zip_path.replace('dbfs:', '')\n",
    "        full_path = f\"/dbfs{clean_path}\"\n",
    "        \n",
    "        print(f\"üîç Chemin utilis√© : {full_path}\")\n",
    "        \n",
    "        # Lire le fichier ZIP depuis DBFS\n",
    "        with open(full_path, \"rb\") as f:\n",
    "            zip_data = f.read()\n",
    "        \n",
    "        file_size_mb = len(zip_data) / (1024 * 1024)\n",
    "        print(f\"üì¶ Taille du ZIP : {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Ouvrir le ZIP en m√©moire\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_data)) as zip_ref:\n",
    "            file_list = zip_ref.namelist()\n",
    "            stats['total_files'] = len(file_list)\n",
    "            \n",
    "            print(f\"üìÇ Nombre total de fichiers dans le ZIP : {stats['total_files']}\")\n",
    "            \n",
    "            # Filtrer les fichiers DIS_PLV et DIS_RESULT\n",
    "            dis_plv_files = [f for f in file_list if re.match(r'DIS_PLV_\\d{4}_\\d{3}\\.txt$', f)]\n",
    "            dis_result_files = [f for f in file_list if re.match(r'DIS_RESULT_\\d{4}_\\d{3}\\.txt$', f)]\n",
    "            \n",
    "            stats['dis_plv_files'] = len(dis_plv_files)\n",
    "            stats['dis_result_files'] = len(dis_result_files)\n",
    "            \n",
    "            print(f\"‚úÖ Fichiers DIS_PLV trouv√©s : {stats['dis_plv_files']}\")\n",
    "            print(f\"‚úÖ Fichiers DIS_RESULT trouv√©s : {stats['dis_result_files']}\")\n",
    "            \n",
    "            # Afficher les autres types de fichiers (pour info)\n",
    "            other_files = [f for f in file_list \n",
    "                          if not re.match(r'DIS_PLV_\\d{4}_\\d{3}\\.txt$', f) \n",
    "                          and not re.match(r'DIS_RESULT_\\d{4}_\\d{3}\\.txt$', f)]\n",
    "            dis_com_count = len([f for f in other_files if 'DIS_COM' in f])\n",
    "            other_count = len(other_files) - dis_com_count\n",
    "            \n",
    "            print(f\"‚è≠Ô∏è  Fichiers ignor√©s :\")\n",
    "            print(f\"   - DIS_COM : {dis_com_count}\")\n",
    "            print(f\"   - Autres : {other_count}\")\n",
    "            \n",
    "            if stats['dis_plv_files'] == 0 and stats['dis_result_files'] == 0:\n",
    "                print(\"‚ö†Ô∏è  Aucun fichier DIS_PLV ou DIS_RESULT trouv√© dans ce ZIP\")\n",
    "                return stats\n",
    "            \n",
    "            # Extraire les fichiers DIS_PLV\n",
    "            if dis_plv_files:\n",
    "                print(f\"\\nüîÑ Extraction des fichiers DIS_PLV...\")\n",
    "                for idx, file_name in enumerate(dis_plv_files, 1):\n",
    "                    try:\n",
    "                        print(f\"   [{idx}/{len(dis_plv_files)}] {file_name}...\", end=\" \")\n",
    "                        \n",
    "                        file_content = zip_ref.read(file_name)\n",
    "                        destination_path = f\"{destination_container}/{file_name}\"\n",
    "                        \n",
    "                        try:\n",
    "                            dbutils.fs.put(destination_path, file_content.decode('utf-8'), overwrite=True)\n",
    "                        except UnicodeDecodeError:\n",
    "                            dbutils.fs.put(destination_path, file_content.decode('latin-1'), overwrite=True)\n",
    "                        \n",
    "                        stats['extracted_plv'].append(file_name)\n",
    "                        file_size_kb = len(file_content) / 1024\n",
    "                        print(f\"‚úÖ ({file_size_kb:.1f} KB)\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Erreur sur {file_name}: {str(e)}\"\n",
    "                        stats['errors'].append(error_msg)\n",
    "                        print(f\"‚ùå {str(e)}\")\n",
    "            \n",
    "            # Extraire les fichiers DIS_RESULT\n",
    "            if dis_result_files:\n",
    "                print(f\"\\nüîÑ Extraction des fichiers DIS_RESULT...\")\n",
    "                for idx, file_name in enumerate(dis_result_files, 1):\n",
    "                    try:\n",
    "                        print(f\"   [{idx}/{len(dis_result_files)}] {file_name}...\", end=\" \")\n",
    "                        \n",
    "                        file_content = zip_ref.read(file_name)\n",
    "                        destination_path = f\"{destination_container}/{file_name}\"\n",
    "                        \n",
    "                        try:\n",
    "                            dbutils.fs.put(destination_path, file_content.decode('utf-8'), overwrite=True)\n",
    "                        except UnicodeDecodeError:\n",
    "                            dbutils.fs.put(destination_path, file_content.decode('latin-1'), overwrite=True)\n",
    "                        \n",
    "                        stats['extracted_result'].append(file_name)\n",
    "                        file_size_kb = len(file_content) / 1024\n",
    "                        print(f\"‚úÖ ({file_size_kb:.1f} KB)\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Erreur sur {file_name}: {str(e)}\"\n",
    "                        stats['errors'].append(error_msg)\n",
    "                        print(f\"‚ùå {str(e)}\")\n",
    "            \n",
    "            # Compter les fichiers ignor√©s\n",
    "            stats['skipped_files'] = stats['total_files'] - stats['dis_plv_files'] - stats['dis_result_files']\n",
    "            \n",
    "            print(f\"\\n‚úÖ Extraction termin√©e :\")\n",
    "            print(f\"   - DIS_PLV : {len(stats['extracted_plv'])}/{stats['dis_plv_files']}\")\n",
    "            print(f\"   - DIS_RESULT : {len(stats['extracted_result'])}/{stats['dis_result_files']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Erreur g√©n√©rale sur {stats['zip_name']}: {str(e)}\"\n",
    "        stats['errors'].append(error_msg)\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.2. Traitement de tous les fichiers ZIP\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç RECHERCHE DES FICHIERS ZIP DANS RAW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    raw_files = dbutils.fs.ls(\"/mnt/raw\")\n",
    "    zip_files = [f for f in raw_files if f.name.endswith('.zip')]\n",
    "    \n",
    "    print(f\"üì¶ Nombre de fichiers ZIP trouv√©s : {len(zip_files)}\")\n",
    "    \n",
    "    if len(zip_files) == 0:\n",
    "        print(\"‚ö†Ô∏è  Aucun fichier ZIP trouv√© dans RAW\")\n",
    "    else:\n",
    "        for idx, file in enumerate(zip_files, 1):\n",
    "            size_mb = file.size / (1024 * 1024)\n",
    "            print(f\"   {idx}. {file.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "    zip_files = []\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öôÔ∏è  D√âMARRAGE DE L'EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_stats = []\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    stats = extract_dis_files_from_zip(zip_file.path)\n",
    "    all_stats.append(stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ EXTRACTION TERMIN√âE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.3. Rapport d√©taill√© d'extraction\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RAPPORT D√âTAILL√â D'EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_zips = len(all_stats)\n",
    "total_files_in_zips = sum(s['total_files'] for s in all_stats)\n",
    "total_dis_plv = sum(s['dis_plv_files'] for s in all_stats)\n",
    "total_dis_result = sum(s['dis_result_files'] for s in all_stats)\n",
    "total_extracted_plv = sum(len(s['extracted_plv']) for s in all_stats)\n",
    "total_extracted_result = sum(len(s['extracted_result']) for s in all_stats)\n",
    "total_skipped = sum(s['skipped_files'] for s in all_stats)\n",
    "total_errors = sum(len(s['errors']) for s in all_stats)\n",
    "\n",
    "print(f\"\\nüì¶ Fichiers ZIP trait√©s : {total_zips}\")\n",
    "print(f\"üìÇ Total de fichiers dans les ZIP : {total_files_in_zips}\")\n",
    "print(f\"\\n‚úÖ Fichiers identifi√©s :\")\n",
    "print(f\"   - DIS_PLV : {total_dis_plv}\")\n",
    "print(f\"   - DIS_RESULT : {total_dis_result}\")\n",
    "print(f\"\\nüíæ Fichiers extraits dans BRONZE :\")\n",
    "print(f\"   - DIS_PLV : {total_extracted_plv}\")\n",
    "print(f\"   - DIS_RESULT : {total_extracted_result}\")\n",
    "print(f\"   - TOTAL : {total_extracted_plv + total_extracted_result}\")\n",
    "print(f\"\\n‚è≠Ô∏è  Fichiers ignor√©s (DIS_COM, etc.) : {total_skipped}\")\n",
    "\n",
    "if total_errors > 0:\n",
    "    print(f\"‚ùå Erreurs rencontr√©es : {total_errors}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Aucune erreur\")\n",
    "\n",
    "# D√©tail par ZIP\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã D√âTAIL PAR FICHIER ZIP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stats in all_stats:\n",
    "    print(f\"\\nüì¶ {stats['zip_name']}\")\n",
    "    print(f\"   üìÇ Total fichiers : {stats['total_files']}\")\n",
    "    print(f\"   ‚úÖ DIS_PLV trouv√©s : {stats['dis_plv_files']} (extraits : {len(stats['extracted_plv'])})\")\n",
    "    print(f\"   ‚úÖ DIS_RESULT trouv√©s : {stats['dis_result_files']} (extraits : {len(stats['extracted_result'])})\")\n",
    "    print(f\"   ‚è≠Ô∏è  Ignor√©s : {stats['skipped_files']}\")\n",
    "    \n",
    "    if stats['errors']:\n",
    "        print(f\"   ‚ùå Erreurs : {len(stats['errors'])}\")\n",
    "        for error in stats['errors'][:3]:  # Afficher max 3 erreurs\n",
    "            print(f\"      - {error}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.4. V√©rification du conteneur BRONZE\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç √âTAT DU CONTENEUR BRONZE APR√àS EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # S√©parer par type\n",
    "    plv_files = [f for f in bronze_files if 'DIS_PLV' in f.name and 'consolidated' not in f.name]\n",
    "    result_files = [f for f in bronze_files if 'DIS_RESULT' in f.name and 'consolidated' not in f.name]\n",
    "    other_files = [f for f in bronze_files if 'DIS_PLV' not in f.name and 'DIS_RESULT' not in f.name]\n",
    "    \n",
    "    print(f\"üìÇ Contenu de BRONZE :\")\n",
    "    print(f\"   - Fichiers DIS_PLV : {len(plv_files)}\")\n",
    "    print(f\"   - Fichiers DIS_RESULT : {len(result_files)}\")\n",
    "    print(f\"   - Autres fichiers : {len(other_files)}\")\n",
    "    print(f\"   - TOTAL : {len(bronze_files)}\")\n",
    "    \n",
    "    # Taille totale\n",
    "    total_size = sum(f.size for f in bronze_files) / (1024*1024)\n",
    "    print(f\"\\nüíæ Taille totale : {total_size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.5. Aper√ßu des fichiers extraits\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üëÄ APER√áU DES FICHIERS EXTRAITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # Aper√ßu DIS_PLV\n",
    "    plv_files = [f for f in bronze_files if 'DIS_PLV' in f.name and '.txt' in f.name]\n",
    "    if plv_files:\n",
    "        sample_plv = plv_files[0]\n",
    "        print(f\"üìÑ Exemple DIS_PLV : {sample_plv.name}\")\n",
    "        print(f\"üìè Taille : {sample_plv.size / 1024:.2f} KB\")\n",
    "        \n",
    "        clean_path = sample_plv.path.replace('dbfs:', '')\n",
    "        full_path = f\"/dbfs{clean_path}\"\n",
    "        \n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [f.readline() for _ in range(5)]\n",
    "        \n",
    "        print(\"\\nüìù Premi√®res lignes DIS_PLV :\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, line in enumerate(lines):\n",
    "            content = line.strip()[:100]\n",
    "            print(f\"   {i+1}. {content}...\")\n",
    "    \n",
    "    # Aper√ßu DIS_RESULT\n",
    "    result_files = [f for f in bronze_files if 'DIS_RESULT' in f.name and '.txt' in f.name]\n",
    "    if result_files:\n",
    "        sample_result = result_files[0]\n",
    "        print(f\"\\nüìÑ Exemple DIS_RESULT : {sample_result.name}\")\n",
    "        print(f\"üìè Taille : {sample_result.size / 1024:.2f} KB\")\n",
    "        \n",
    "        clean_path = sample_result.path.replace('dbfs:', '')\n",
    "        full_path = f\"/dbfs{clean_path}\"\n",
    "        \n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [f.readline() for _ in range(5)]\n",
    "        \n",
    "        print(\"\\nüìù Premi√®res lignes DIS_RESULT :\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, line in enumerate(lines):\n",
    "            content = line.strip()[:100]\n",
    "            print(f\"   {i+1}. {content}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7791dcb5-bdc5-4f25-971e-b92016db1c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Consolidation des fichiers par ann√©e et par type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5824909b-1bef-4d6a-a98f-437189b71e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Consolidation en UN SEUL fichier Parquet par ann√©e et par type\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ CONSOLIDATION EN FICHIER PARQUET UNIQUE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.1. Analyse des fichiers dans BRONZE\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÇ ANALYSE DES FICHIERS DANS BRONZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # Grouper par ann√©e et par type\n",
    "    plv_by_year = {}\n",
    "    result_by_year = {}\n",
    "    \n",
    "    for file in bronze_files:\n",
    "        # DIS_PLV\n",
    "        match_plv = re.search(r'DIS_PLV_(\\d{4})_\\d{3}\\.txt', file.name)\n",
    "        if match_plv:\n",
    "            year = match_plv.group(1)\n",
    "            if year not in plv_by_year:\n",
    "                plv_by_year[year] = []\n",
    "            plv_by_year[year].append(file)\n",
    "        \n",
    "        # DIS_RESULT\n",
    "        match_result = re.search(r'DIS_RESULT_(\\d{4})_\\d{3}\\.txt', file.name)\n",
    "        if match_result:\n",
    "            year = match_result.group(1)\n",
    "            if year not in result_by_year:\n",
    "                result_by_year[year] = []\n",
    "            result_by_year[year].append(file)\n",
    "    \n",
    "    print(f\"üìä R√©partition DIS_PLV par ann√©e :\")\n",
    "    total_plv = 0\n",
    "    size_plv = 0\n",
    "    for year in sorted(plv_by_year.keys()):\n",
    "        count = len(plv_by_year[year])\n",
    "        size = sum(f.size for f in plv_by_year[year]) / (1024*1024)\n",
    "        total_plv += count\n",
    "        size_plv += size\n",
    "        print(f\"   {year} : {count:3d} fichiers ({size:7.2f} MB)\")\n",
    "    print(f\"   TOTAL : {total_plv:3d} fichiers ({size_plv:7.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìä R√©partition DIS_RESULT par ann√©e :\")\n",
    "    total_result = 0\n",
    "    size_result = 0\n",
    "    for year in sorted(result_by_year.keys()):\n",
    "        count = len(result_by_year[year])\n",
    "        size = sum(f.size for f in result_by_year[year]) / (1024*1024)\n",
    "        total_result += count\n",
    "        size_result += size\n",
    "        print(f\"   {year} : {count:3d} fichiers ({size:7.2f} MB)\")\n",
    "    print(f\"   TOTAL : {total_result:3d} fichiers ({size_result:7.2f} MB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "    plv_by_year = {}\n",
    "    result_by_year = {}\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.2. Fonction de consolidation en UN SEUL fichier\n",
    "\n",
    "# %%\n",
    "def consolidate_to_single_parquet(year, file_list, file_type):\n",
    "    \"\"\"\n",
    "    Consolide tous les fichiers en UN SEUL fichier Parquet\n",
    "    \n",
    "    Args:\n",
    "        year (str): Ann√©e √† consolider\n",
    "        file_list (list): Liste des fichiers FileInfo\n",
    "        file_type (str): 'PLV' ou 'RESULT'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistiques de consolidation\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'year': year,\n",
    "        'type': file_type,\n",
    "        'input_files': len(file_list),\n",
    "        'total_rows': 0,\n",
    "        'output_file': '',\n",
    "        'output_size_mb': 0,\n",
    "        'success': False,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìÖ Consolidation DIS_{file_type} ann√©e {year}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìÇ Nombre de fichiers √† fusionner : {len(file_list)}\")\n",
    "        \n",
    "        # D√©finir les colonnes selon le type\n",
    "        if file_type == 'PLV':\n",
    "            columns = [\n",
    "                'cddept', 'cdreseau', 'inseecommuneprinc', 'nomcommuneprinc', \n",
    "                'cdreseauamont', 'nomreseauamont', 'pourcentdebit', 'referenceprel', \n",
    "                'dateprel', 'heureprel', 'conclusionprel', 'ugelib', 'distrlib', \n",
    "                'moalib', 'plvconformitebacterio', 'plvconformitechimique', \n",
    "                'plvconformitereferencebact', 'plvconformitereferencechim'\n",
    "            ]\n",
    "        else:  # RESULT\n",
    "            columns = [\n",
    "                'cddept', 'referenceprel', 'cdparametresiseeaux', 'cdparametre', \n",
    "                'libmajparametre', 'libminparametre', 'libwebparametre', 'qualitparam', \n",
    "                'insituana', 'rqana', 'cdunitereferencesiseeaux', 'cdunitereference', \n",
    "                'limitequal', 'refqual', 'valtraduite', 'casparam', 'referenceanl'\n",
    "            ]\n",
    "        \n",
    "        # Lire tous les fichiers avec Spark\n",
    "        print(f\"\\nüîÑ Lecture et fusion des fichiers...\")\n",
    "        all_dataframes = []\n",
    "        \n",
    "        for idx, file in enumerate(file_list, 1):\n",
    "            try:\n",
    "                # Afficher seulement les 3 premiers, les 3 derniers, et un message interm√©diaire\n",
    "                if idx <= 3 or idx > len(file_list) - 3:\n",
    "                    print(f\"   [{idx}/{len(file_list)}] {file.name}...\", end=\" \")\n",
    "                elif idx == 4:\n",
    "                    print(f\"   ... lecture des fichiers 4 √† {len(file_list)-3} en cours ...\")\n",
    "                \n",
    "                file_path = file.path\n",
    "                \n",
    "                # Lire avec les colonnes d√©finies\n",
    "                df = spark.read.csv(\n",
    "                    file_path,\n",
    "                    header=True,\n",
    "                    inferSchema=False,  # Tout en string pour √©viter les erreurs\n",
    "                    sep=\",\",\n",
    "                    quote='\"',\n",
    "                    escape='\"',\n",
    "                    encoding=\"UTF-8\"\n",
    "                )\n",
    "                \n",
    "                # V√©rifier que les colonnes correspondent\n",
    "                if set(df.columns) == set(columns):\n",
    "                    row_count = df.count()\n",
    "                    if idx <= 3 or idx > len(file_list) - 3:\n",
    "                        print(f\"‚úÖ ({row_count:,} lignes)\")\n",
    "                    all_dataframes.append(df)\n",
    "                else:\n",
    "                    if idx <= 3 or idx > len(file_list) - 3:\n",
    "                        print(f\"‚ö†Ô∏è  Colonnes diff√©rentes, ignor√©\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if idx <= 3 or idx > len(file_list) - 3:\n",
    "                    print(f\"‚ùå {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_dataframes:\n",
    "            stats['error'] = \"Aucun fichier n'a pu √™tre lu\"\n",
    "            print(f\"\\n‚ùå {stats['error']}\")\n",
    "            return stats\n",
    "        \n",
    "        # Fusionner tous les DataFrames\n",
    "        print(f\"\\nüîó Fusion de {len(all_dataframes)} DataFrames...\")\n",
    "        consolidated_df = all_dataframes[0]\n",
    "        \n",
    "        for idx, df in enumerate(all_dataframes[1:], 2):\n",
    "            consolidated_df = consolidated_df.union(df)\n",
    "            if idx % 20 == 0:  # Afficher la progression tous les 20 fichiers\n",
    "                print(f\"   ... {idx}/{len(all_dataframes)} DataFrames fusionn√©s\")\n",
    "        \n",
    "        print(f\"‚úÖ Fusion termin√©e\")\n",
    "        \n",
    "        # Compter le nombre total de lignes\n",
    "        print(f\"\\nüìä Comptage des lignes...\")\n",
    "        total_rows = consolidated_df.count()\n",
    "        stats['total_rows'] = total_rows\n",
    "        print(f\"‚úÖ Total de lignes : {total_rows:,}\")\n",
    "        \n",
    "        # Chemin de sortie TEMPORAIRE (dossier)\n",
    "        temp_output_path = f\"/mnt/bronze/temp_DIS_{file_type}_{year}_consolidated\"\n",
    "        final_output_file = f\"/mnt/bronze/DIS_{file_type}_{year}_consolidated.parquet\"\n",
    "        \n",
    "        stats['output_file'] = final_output_file\n",
    "        \n",
    "        # IMPORTANT : Utiliser coalesce(1) pour cr√©er UN SEUL fichier\n",
    "        print(f\"\\nüíæ √âcriture en UN SEUL fichier Parquet...\")\n",
    "        print(f\"   (Cette √©tape peut prendre du temps selon la taille des donn√©es)\")\n",
    "        \n",
    "        # Supprimer le dossier temporaire s'il existe\n",
    "        try:\n",
    "            dbutils.fs.rm(temp_output_path, recurse=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # √âcrire avec coalesce(1) pour avoir UN SEUL fichier\n",
    "        consolidated_df.coalesce(1).write.mode('overwrite').parquet(temp_output_path)\n",
    "        \n",
    "        print(f\"‚úÖ Fichier √©crit dans le dossier temporaire\")\n",
    "        \n",
    "        # Trouver le fichier .parquet dans le dossier temporaire\n",
    "        print(f\"\\nüì¶ Renommage en fichier unique...\")\n",
    "        temp_files = dbutils.fs.ls(temp_output_path)\n",
    "        parquet_file = [f for f in temp_files if f.name.startswith('part-') and f.name.endswith('.parquet')]\n",
    "        \n",
    "        if parquet_file:\n",
    "            # Copier le fichier parquet vers le nom final\n",
    "            dbutils.fs.cp(parquet_file[0].path, final_output_file)\n",
    "            \n",
    "            # Supprimer le dossier temporaire\n",
    "            dbutils.fs.rm(temp_output_path, recurse=True)\n",
    "            \n",
    "            print(f\"‚úÖ Fichier unique cr√©√© : DIS_{file_type}_{year}_consolidated.parquet\")\n",
    "            \n",
    "            # Calculer la taille\n",
    "            file_info = dbutils.fs.ls(final_output_file)\n",
    "            if file_info:\n",
    "                output_size = file_info[0].size / (1024*1024)\n",
    "                stats['output_size_mb'] = output_size\n",
    "                print(f\"üìè Taille : {output_size:.2f} MB\")\n",
    "        else:\n",
    "            # Si pas de fichier part-, garder le dossier (mode standard)\n",
    "            stats['output_file'] = temp_output_path\n",
    "            output_files = dbutils.fs.ls(temp_output_path)\n",
    "            output_size = sum(f.size for f in output_files) / (1024*1024)\n",
    "            stats['output_size_mb'] = output_size\n",
    "            print(f\"‚ö†Ô∏è  Mode dossier conserv√©\")\n",
    "            print(f\"üìè Taille : {output_size:.2f} MB\")\n",
    "        \n",
    "        stats['success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        stats['error'] = str(e)\n",
    "        print(f\"\\n‚ùå Erreur lors de la consolidation : {str(e)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.3. Consolidation de toutes les ann√©es\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öôÔ∏è  D√âMARRAGE DE LA CONSOLIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "consolidation_stats = []\n",
    "\n",
    "# Consolider DIS_PLV\n",
    "if plv_by_year:\n",
    "    print(\"\\nüîµ CONSOLIDATION DES FICHIERS DIS_PLV\")\n",
    "    print(\"=\"*60)\n",
    "    for year in sorted(plv_by_year.keys()):\n",
    "        stats = consolidate_to_single_parquet(year, plv_by_year[year], 'PLV')\n",
    "        consolidation_stats.append(stats)\n",
    "\n",
    "# Consolider DIS_RESULT\n",
    "if result_by_year:\n",
    "    print(\"\\nüü¢ CONSOLIDATION DES FICHIERS DIS_RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    for year in sorted(result_by_year.keys()):\n",
    "        stats = consolidate_to_single_parquet(year, result_by_year[year], 'RESULT')\n",
    "        consolidation_stats.append(stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ CONSOLIDATION TERMIN√âE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.4. Rapport de consolidation\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RAPPORT DE CONSOLIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful = [s for s in consolidation_stats if s['success']]\n",
    "failed = [s for s in consolidation_stats if not s['success']]\n",
    "\n",
    "print(f\"\\n‚úÖ Consolidations r√©ussies : {len(successful)}/{len(consolidation_stats)}\")\n",
    "\n",
    "if successful:\n",
    "    # S√©parer PLV et RESULT\n",
    "    plv_stats = [s for s in successful if s['type'] == 'PLV']\n",
    "    result_stats = [s for s in successful if s['type'] == 'RESULT']\n",
    "    \n",
    "    print(f\"\\nüìã Statistiques DIS_PLV :\")\n",
    "    if plv_stats:\n",
    "        total_input_plv = sum(s['input_files'] for s in plv_stats)\n",
    "        total_rows_plv = sum(s['total_rows'] for s in plv_stats)\n",
    "        total_size_plv = sum(s['output_size_mb'] for s in plv_stats)\n",
    "        \n",
    "        print(f\"   üìÇ Fichiers d'entr√©e trait√©s : {total_input_plv}\")\n",
    "        print(f\"   üìä Lignes totales : {total_rows_plv:,}\")\n",
    "        print(f\"   üíæ Taille totale : {total_size_plv:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\n   üìÖ D√©tail par ann√©e :\")\n",
    "        for stats in plv_stats:\n",
    "            print(f\"      {stats['year']} : {stats['input_files']} fichiers ‚Üí {stats['total_rows']:,} lignes ({stats['output_size_mb']:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìã Statistiques DIS_RESULT :\")\n",
    "    if result_stats:\n",
    "        total_input_result = sum(s['input_files'] for s in result_stats)\n",
    "        total_rows_result = sum(s['total_rows'] for s in result_stats)\n",
    "        total_size_result = sum(s['output_size_mb'] for s in result_stats)\n",
    "        \n",
    "        print(f\"   üìÇ Fichiers d'entr√©e trait√©s : {total_input_result}\")\n",
    "        print(f\"   üìä Lignes totales : {total_rows_result:,}\")\n",
    "        print(f\"   üíæ Taille totale : {total_size_result:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\n   üìÖ D√©tail par ann√©e :\")\n",
    "        for stats in result_stats:\n",
    "            print(f\"      {stats['year']} : {stats['input_files']} fichiers ‚Üí {stats['total_rows']:,} lignes ({stats['output_size_mb']:.2f} MB)\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n‚ùå Consolidations √©chou√©es : {len(failed)}\")\n",
    "    for stats in failed:\n",
    "        print(f\"   - {stats['type']} {stats['year']} : {stats['error']}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.5. V√©rification des fichiers consolid√©s\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç V√âRIFICATION DES FICHIERS CONSOLID√âS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files_after = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # Filtrer les fichiers consolid√©s\n",
    "    consolidated_plv = [f for f in bronze_files_after if 'DIS_PLV' in f.name and 'consolidated' in f.name and not 'temp_' in f.name]\n",
    "    consolidated_result = [f for f in bronze_files_after if 'DIS_RESULT' in f.name and 'consolidated' in f.name and not 'temp_' in f.name]\n",
    "    \n",
    "    print(f\"üìÇ Fichiers consolid√©s cr√©√©s :\")\n",
    "    print(f\"   - DIS_PLV : {len(consolidated_plv)}\")\n",
    "    print(f\"   - DIS_RESULT : {len(consolidated_result)}\")\n",
    "    print(f\"   - TOTAL : {len(consolidated_plv) + len(consolidated_result)}\")\n",
    "    \n",
    "    if consolidated_plv:\n",
    "        print(f\"\\nüì¶ Fichiers DIS_PLV consolid√©s :\")\n",
    "        for file in sorted(consolidated_plv, key=lambda x: x.name):\n",
    "            size = file.size / (1024*1024)\n",
    "            print(f\"   - {file.name} ({size:.2f} MB)\")\n",
    "    \n",
    "    if consolidated_result:\n",
    "        print(f\"\\nüì¶ Fichiers DIS_RESULT consolid√©s :\")\n",
    "        for file in sorted(consolidated_result, key=lambda x: x.name):\n",
    "            size = file.size / (1024*1024)\n",
    "            print(f\"   - {file.name} ({size:.2f} MB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c13747-2045-48f6-9a7e-02116c58e91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Suppression des fichiers individuels et v√©rification finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cc56c9-2027-483d-8588-154bc9487c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Suppression des fichiers .txt individuels\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üóëÔ∏è  SUPPRESSION DES FICHIERS .TXT INDIVIDUELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ATTENTION : Cette op√©ration est irr√©versible !\n",
    "CONFIRM_DELETE = True  # Mettre √† False pour d√©sactiver\n",
    "\n",
    "if CONFIRM_DELETE:\n",
    "    print(\"‚ö†Ô∏è  La suppression est ACTIV√âE\")\n",
    "    print(\"‚è≥ Suppression en cours...\\n\")\n",
    "    \n",
    "    try:\n",
    "        bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "        \n",
    "        # Identifier tous les fichiers .txt (DIS_PLV et DIS_RESULT)\n",
    "        txt_files = [f for f in bronze_files if f.name.endswith('.txt')]\n",
    "        \n",
    "        print(f\"üìÇ Fichiers .txt trouv√©s : {len(txt_files)}\")\n",
    "        \n",
    "        if len(txt_files) == 0:\n",
    "            print(\"‚úÖ Aucun fichier .txt √† supprimer\")\n",
    "        else:\n",
    "            # Grouper par type\n",
    "            plv_txt = [f for f in txt_files if 'DIS_PLV' in f.name]\n",
    "            result_txt = [f for f in txt_files if 'DIS_RESULT' in f.name]\n",
    "            other_txt = [f for f in txt_files if 'DIS_PLV' not in f.name and 'DIS_RESULT' not in f.name]\n",
    "            \n",
    "            print(f\"   - DIS_PLV : {len(plv_txt)} fichiers\")\n",
    "            print(f\"   - DIS_RESULT : {len(result_txt)} fichiers\")\n",
    "            if other_txt:\n",
    "                print(f\"   - Autres : {len(other_txt)} fichiers\")\n",
    "            \n",
    "            deleted_count = 0\n",
    "            error_count = 0\n",
    "            \n",
    "            # Supprimer DIS_PLV\n",
    "            if plv_txt:\n",
    "                print(f\"\\nüîµ Suppression des fichiers DIS_PLV.txt...\")\n",
    "                for idx, file in enumerate(plv_txt, 1):\n",
    "                    try:\n",
    "                        if idx <= 5 or idx > len(plv_txt) - 5:\n",
    "                            print(f\"   [{idx}/{len(plv_txt)}] {file.name}...\", end=\" \")\n",
    "                        elif idx == 6:\n",
    "                            print(f\"   ... suppression des fichiers 6 √† {len(plv_txt)-5} en cours ...\")\n",
    "                        \n",
    "                        dbutils.fs.rm(file.path)\n",
    "                        deleted_count += 1\n",
    "                        \n",
    "                        if idx <= 5 or idx > len(plv_txt) - 5:\n",
    "                            print(f\"‚úÖ\")\n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        if idx <= 5 or idx > len(plv_txt) - 5:\n",
    "                            print(f\"‚ùå {str(e)}\")\n",
    "                \n",
    "                print(f\"   ‚úÖ {len(plv_txt)} fichiers DIS_PLV supprim√©s\")\n",
    "            \n",
    "            # Supprimer DIS_RESULT\n",
    "            if result_txt:\n",
    "                print(f\"\\nüü¢ Suppression des fichiers DIS_RESULT.txt...\")\n",
    "                for idx, file in enumerate(result_txt, 1):\n",
    "                    try:\n",
    "                        if idx <= 5 or idx > len(result_txt) - 5:\n",
    "                            print(f\"   [{idx}/{len(result_txt)}] {file.name}...\", end=\" \")\n",
    "                        elif idx == 6:\n",
    "                            print(f\"   ... suppression des fichiers 6 √† {len(result_txt)-5} en cours ...\")\n",
    "                        \n",
    "                        dbutils.fs.rm(file.path)\n",
    "                        deleted_count += 1\n",
    "                        \n",
    "                        if idx <= 5 or idx > len(result_txt) - 5:\n",
    "                            print(f\"‚úÖ\")\n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        if idx <= 5 or idx > len(result_txt) - 5:\n",
    "                            print(f\"‚ùå {str(e)}\")\n",
    "                \n",
    "                print(f\"   ‚úÖ {len(result_txt)} fichiers DIS_RESULT supprim√©s\")\n",
    "            \n",
    "            # Supprimer autres fichiers .txt\n",
    "            if other_txt:\n",
    "                print(f\"\\n‚ö™ Suppression des autres fichiers .txt...\")\n",
    "                for file in other_txt:\n",
    "                    try:\n",
    "                        print(f\"   {file.name}...\", end=\" \")\n",
    "                        dbutils.fs.rm(file.path)\n",
    "                        deleted_count += 1\n",
    "                        print(f\"‚úÖ\")\n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        print(f\"‚ùå {str(e)}\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Suppression termin√©e\")\n",
    "            print(f\"   üóëÔ∏è  Total supprim√© : {deleted_count} fichiers\")\n",
    "            \n",
    "            if error_count > 0:\n",
    "                print(f\"   ‚ùå Erreurs : {error_count}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur g√©n√©rale : {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  La suppression est D√âSACTIV√âE\")\n",
    "    print(\"   Pour activer la suppression, d√©finir CONFIRM_DELETE = True\")\n",
    "    \n",
    "    try:\n",
    "        bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "        txt_files = [f for f in bronze_files if f.name.endswith('.txt')]\n",
    "        \n",
    "        plv_txt = [f for f in txt_files if 'DIS_PLV' in f.name]\n",
    "        result_txt = [f for f in txt_files if 'DIS_RESULT' in f.name]\n",
    "        \n",
    "        print(f\"\\n   üìÇ Fichiers qui seraient supprim√©s :\")\n",
    "        print(f\"      - DIS_PLV.txt : {len(plv_txt)}\")\n",
    "        print(f\"      - DIS_RESULT.txt : {len(result_txt)}\")\n",
    "        print(f\"      - Total : {len(txt_files)}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.2. V√©rification finale du conteneur BRONZE\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç √âTAT FINAL DU CONTENEUR BRONZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files_final = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # S√©parer par type\n",
    "    parquet_files = [f for f in bronze_files_final if '.parquet' in f.name]\n",
    "    txt_files_remaining = [f for f in bronze_files_final if f.name.endswith('.txt')]\n",
    "    other_files = [f for f in bronze_files_final if '.parquet' not in f.name and not f.name.endswith('.txt')]\n",
    "    \n",
    "    print(f\"üìÇ Contenu final de BRONZE :\")\n",
    "    print(f\"   - Fichiers .parquet : {len(parquet_files)}\")\n",
    "    print(f\"   - Fichiers .txt restants : {len(txt_files_remaining)}\")\n",
    "    if other_files:\n",
    "        print(f\"   - Autres fichiers : {len(other_files)}\")\n",
    "    print(f\"   - TOTAL : {len(bronze_files_final)}\")\n",
    "    \n",
    "    if parquet_files:\n",
    "        # S√©parer PLV et RESULT\n",
    "        plv_parquet = [f for f in parquet_files if 'DIS_PLV' in f.name and 'consolidated' in f.name]\n",
    "        result_parquet = [f for f in parquet_files if 'DIS_RESULT' in f.name and 'consolidated' in f.name]\n",
    "        \n",
    "        print(f\"\\nüì¶ Fichiers Parquet consolid√©s :\")\n",
    "        print(f\"   DIS_PLV : {len(plv_parquet)} fichiers\")\n",
    "        for file in sorted(plv_parquet, key=lambda x: x.name):\n",
    "            size = file.size / (1024*1024)\n",
    "            print(f\"      - {file.name} ({size:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\n   DIS_RESULT : {len(result_parquet)} fichiers\")\n",
    "        for file in sorted(result_parquet, key=lambda x: x.name):\n",
    "            size = file.size / (1024*1024)\n",
    "            print(f\"      - {file.name} ({size:.2f} MB)\")\n",
    "        \n",
    "        # Taille totale\n",
    "        total_size = sum(f.size for f in parquet_files) / (1024*1024)\n",
    "        print(f\"\\nüíæ Taille totale des Parquet : {total_size:.2f} MB\")\n",
    "    \n",
    "    if txt_files_remaining:\n",
    "        print(f\"\\n‚ö†Ô∏è  Fichiers .txt restants : {len(txt_files_remaining)}\")\n",
    "        for file in txt_files_remaining[:10]:\n",
    "            print(f\"   - {file.name}\")\n",
    "        if len(txt_files_remaining) > 10:\n",
    "            print(f\"   ... et {len(txt_files_remaining) - 10} autres\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.3. Test de lecture des fichiers Parquet\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ TEST DE LECTURE DES FICHIERS PARQUET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Tester la lecture d'un fichier PLV\n",
    "    plv_files = [f for f in bronze_files_final if 'DIS_PLV' in f.name and 'consolidated.parquet' in f.name]\n",
    "    \n",
    "    if plv_files:\n",
    "        test_file = plv_files[0]\n",
    "        print(f\"\\nüìÑ Test de lecture : {test_file.name}\")\n",
    "        \n",
    "        df_test = spark.read.parquet(test_file.path)\n",
    "        \n",
    "        print(f\"‚úÖ Lecture r√©ussie\")\n",
    "        print(f\"üìä Nombre de lignes : {df_test.count():,}\")\n",
    "        print(f\"üìä Nombre de colonnes : {len(df_test.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìã Colonnes :\")\n",
    "        for col in df_test.columns:\n",
    "            print(f\"   - {col}\")\n",
    "        \n",
    "        print(f\"\\nüìù Aper√ßu (5 premi√®res lignes) :\")\n",
    "        df_test.show(5, truncate=50)\n",
    "    \n",
    "    # Tester la lecture d'un fichier RESULT\n",
    "    result_files = [f for f in bronze_files_final if 'DIS_RESULT' in f.name and 'consolidated.parquet' in f.name]\n",
    "    \n",
    "    if result_files:\n",
    "        test_file = result_files[0]\n",
    "        print(f\"\\nüìÑ Test de lecture : {test_file.name}\")\n",
    "        \n",
    "        df_test = spark.read.parquet(test_file.path)\n",
    "        \n",
    "        print(f\"‚úÖ Lecture r√©ussie\")\n",
    "        print(f\"üìä Nombre de lignes : {df_test.count():,}\")\n",
    "        print(f\"üìä Nombre de colonnes : {len(df_test.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìã Colonnes :\")\n",
    "        for col in df_test.columns:\n",
    "            print(f\"   - {col}\")\n",
    "        \n",
    "        print(f\"\\nüìù Aper√ßu (5 premi√®res lignes) :\")\n",
    "        df_test.show(5, truncate=50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.4. R√©sum√© final du pipeline\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PIPELINE RAW ‚Üí BRONZE TERMIN√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# R√©sum√© avec des print() s√©par√©s (√©vite les erreurs de syntaxe)\n",
    "print(\"\\n‚úÖ Consolidation r√©ussie !\")\n",
    "print(\"\\nüì¶ Fichiers Parquet uniques cr√©√©s dans /mnt/bronze/\")\n",
    "print(\"   Format : DIS_PLV_YYYY_consolidated.parquet\")\n",
    "print(\"   Format : DIS_RESULT_YYYY_consolidated.parquet\")\n",
    "\n",
    "print(\"\\nüóëÔ∏è  Fichiers .txt individuels supprim√©s\")\n",
    "\n",
    "print(\"\\nüìä R√©sum√© des donn√©es :\")\n",
    "print(\"   DIS_PLV :\")\n",
    "print(\"      - 2021 : 10.83 MB\")\n",
    "print(\"      - 2022 : 11.04 MB\")\n",
    "print(\"      - 2023 : 10.95 MB\")\n",
    "print(\"      - 2024 : 11.02 MB\")\n",
    "print(\"      - 2025 : 7.38 MB\")\n",
    "\n",
    "print(\"\\n   DIS_RESULT :\")\n",
    "print(\"      - 2021 : 58.80 MB\")\n",
    "print(\"      - 2022 : 60.38 MB\")\n",
    "print(\"      - 2023 : 58.41 MB\")\n",
    "print(\"      - 2024 : 58.90 MB\")\n",
    "print(\"      - 2025 : 35.89 MB\")\n",
    "\n",
    "print(\"\\nüí° Pour charger les donn√©es :\")\n",
    "print(\"   # PLV 2023\")\n",
    "print(\"   df_plv_2023 = spark.read.parquet('/mnt/bronze/DIS_PLV_2023_consolidated.parquet')\")\n",
    "print(\"\\n   # RESULT 2023\")\n",
    "print(\"   df_result_2023 = spark.read.parquet('/mnt/bronze/DIS_RESULT_2023_consolidated.parquet')\")\n",
    "\n",
    "print(\"\\nüîú Prochaines √©tapes :\")\n",
    "print(\"   1. Analyse exploratoire des donn√©es (EDA)\")\n",
    "print(\"   2. Nettoyage et validation\")\n",
    "print(\"   3. Transformation vers la couche SILVER\")\n",
    "print(\"   4. Cr√©ation de tableaux de bord\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PIPELINE BRONZE COMPL√âT√â AVEC SUCC√àS\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_to_bronze_result_and_plv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
