{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81119334-26b8-405c-a4d6-b59265cec18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Configuration initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcf7194-7b39-4b81-afa1-58fde8b5b6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import year, month, to_date, col\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß CONFIGURATION INITIALE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Variables d'environnement\n",
    "storage_account_name = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "\n",
    "if storage_account_name and storage_account_key:\n",
    "    # Configuration Spark\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "        storage_account_key\n",
    "    )\n",
    "    print(\"‚úÖ Configuration Spark effectu√©e\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Variables d'environnement non configur√©es\")\n",
    "\n",
    "# Nom de la base de donn√©es\n",
    "DATABASE_NAME = \"eau_potable\"\n",
    "\n",
    "print(f\"üì¶ Base de donn√©es : {DATABASE_NAME}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "645f01fc-6d66-4c4f-9875-24354bad8c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. V√©rification des fichiers Parquet dans BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb08210-4dbf-47a7-b88e-8ac90bb9d346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÇ V√âRIFICATION DES FICHIERS PARQUET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    bronze_files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "    \n",
    "    # Filtrer les fichiers Parquet consolid√©s\n",
    "    plv_files = sorted([f for f in bronze_files if 'DIS_PLV' in f.name and 'consolidated.parquet' in f.name])\n",
    "    result_files = sorted([f for f in bronze_files if 'DIS_RESULT' in f.name and 'consolidated.parquet' in f.name])\n",
    "    \n",
    "    print(f\"\\nüì¶ Fichiers DIS_PLV trouv√©s : {len(plv_files)}\")\n",
    "    for file in plv_files:\n",
    "        size = file.size / (1024*1024)\n",
    "        year = file.name.split('_')[2]\n",
    "        print(f\"   ‚úÖ {year} : {file.name} ({size:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Fichiers DIS_RESULT trouv√©s : {len(result_files)}\")\n",
    "    for file in result_files:\n",
    "        size = file.size / (1024*1024)\n",
    "        year = file.name.split('_')[2]\n",
    "        print(f\"   ‚úÖ {year} : {file.name} ({size:.2f} MB)\")\n",
    "    \n",
    "    if not plv_files and not result_files:\n",
    "        raise Exception(\"‚ùå Aucun fichier Parquet consolid√© trouv√© dans /mnt/bronze\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7def77cc-de84-46fb-aa16-b61fa5e3bcb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Cr√©ation de la base de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9f460a-389b-46bd-a4d4-e3d1588df62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üóÑÔ∏è  CR√âATION DE LA BASE DE DONN√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr√©er la base de donn√©es si elle n'existe pas\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "print(f\"‚úÖ Base de donn√©es '{DATABASE_NAME}' cr√©√©e/v√©rifi√©e\")\n",
    "\n",
    "# D√©finir comme base par d√©faut\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "print(f\"‚úÖ Base de donn√©es '{DATABASE_NAME}' activ√©e\")\n",
    "\n",
    "# Afficher les bases existantes\n",
    "print(\"\\nüìã Bases de donn√©es disponibles :\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9c52e0-b35a-4149-9ba8-a44df3d580f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Cr√©ation des tables DIS_PLV par ann√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c859b55a-0850-4b9c-b3b2-fb14dfa2df0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CR√âATION DE LA TABLE DIS_PLV (TOUTES ANN√âES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "try:\n",
    "    all_plv_dfs = []\n",
    "    plv_stats = []\n",
    "    \n",
    "    # Lire tous les fichiers DIS_PLV et ajouter une colonne \"annee\"\n",
    "    for file in plv_files:\n",
    "        year = file.name.split('_')[2]\n",
    "        \n",
    "        print(f\"\\nüîÑ Lecture : DIS_PLV {year}\")\n",
    "        print(f\"   üìÇ Fichier : {file.name}\")\n",
    "        \n",
    "        # Lire le fichier Parquet\n",
    "        df = spark.read.parquet(file.path)\n",
    "        \n",
    "        # Ajouter la colonne \"annee\"\n",
    "        df = df.withColumn(\"annee\", lit(year))\n",
    "        \n",
    "        row_count = df.count()\n",
    "        print(f\"   üìä Lignes : {row_count:,}\")\n",
    "        \n",
    "        all_plv_dfs.append(df)\n",
    "        plv_stats.append({'year': year, 'rows': row_count})\n",
    "    \n",
    "    if all_plv_dfs:\n",
    "        print(f\"\\nüîó Fusion de {len(all_plv_dfs)} fichiers DIS_PLV...\")\n",
    "        \n",
    "        # Fusionner tous les DataFrames\n",
    "        df_plv_consolidated = all_plv_dfs[0]\n",
    "        for df in all_plv_dfs[1:]:\n",
    "            df_plv_consolidated = df_plv_consolidated.union(df)\n",
    "        \n",
    "        # Compter le total de lignes\n",
    "        total_plv_rows = df_plv_consolidated.count()\n",
    "        print(f\"‚úÖ Total de lignes apr√®s fusion : {total_plv_rows:,}\")\n",
    "        \n",
    "        # Cr√©er la table unique\n",
    "        table_name = \"dis_plv\"\n",
    "        print(f\"\\nüíæ Cr√©ation de la table '{table_name}'...\")\n",
    "        \n",
    "        df_plv_consolidated.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.{table_name}\")\n",
    "        \n",
    "        print(f\"‚úÖ Table '{DATABASE_NAME}.{table_name}' cr√©√©e avec succ√®s\")\n",
    "        print(f\"   üìä Lignes totales : {total_plv_rows:,}\")\n",
    "        print(f\"   üìä Colonnes : {len(df_plv_consolidated.columns)}\")\n",
    "        print(f\"   üìÖ Ann√©es incluses : {', '.join([s['year'] for s in plv_stats])}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun fichier DIS_PLV √† traiter\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0800f6b3-462a-44c0-b01d-cad2e67ec8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Cr√©ation des tables DIS_RESULT par ann√©e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94ecd69-3db9-40f7-b235-3d57bab2d078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CR√âATION DE LA TABLE DIS_RESULT (TOUTES ANN√âES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    all_result_dfs = []\n",
    "    result_stats = []\n",
    "    \n",
    "    # Lire tous les fichiers DIS_RESULT et ajouter une colonne \"annee\"\n",
    "    for file in result_files:\n",
    "        year = file.name.split('_')[2]\n",
    "        \n",
    "        print(f\"\\nüîÑ Lecture : DIS_RESULT {year}\")\n",
    "        print(f\"   üìÇ Fichier : {file.name}\")\n",
    "        \n",
    "        # Lire le fichier Parquet\n",
    "        df = spark.read.parquet(file.path)\n",
    "        \n",
    "        # Ajouter la colonne \"annee\"\n",
    "        df = df.withColumn(\"annee\", lit(year))\n",
    "        \n",
    "        row_count = df.count()\n",
    "        print(f\"   üìä Lignes : {row_count:,}\")\n",
    "        \n",
    "        all_result_dfs.append(df)\n",
    "        result_stats.append({'year': year, 'rows': row_count})\n",
    "    \n",
    "    if all_result_dfs:\n",
    "        print(f\"\\nüîó Fusion de {len(all_result_dfs)} fichiers DIS_RESULT...\")\n",
    "        \n",
    "        # Fusionner tous les DataFrames\n",
    "        df_result_consolidated = all_result_dfs[0]\n",
    "        for df in all_result_dfs[1:]:\n",
    "            df_result_consolidated = df_result_consolidated.union(df)\n",
    "        \n",
    "        # Compter le total de lignes\n",
    "        total_result_rows = df_result_consolidated.count()\n",
    "        print(f\"‚úÖ Total de lignes apr√®s fusion : {total_result_rows:,}\")\n",
    "        \n",
    "        # Cr√©er la table unique\n",
    "        table_name = \"dis_result\"\n",
    "        print(f\"\\nüíæ Cr√©ation de la table '{table_name}'...\")\n",
    "        \n",
    "        df_result_consolidated.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.{table_name}\")\n",
    "        \n",
    "        print(f\"‚úÖ Table '{DATABASE_NAME}.{table_name}' cr√©√©e avec succ√®s\")\n",
    "        print(f\"   üìä Lignes totales : {total_result_rows:,}\")\n",
    "        print(f\"   üìä Colonnes : {len(df_result_consolidated.columns)}\")\n",
    "        print(f\"   üìÖ Ann√©es incluses : {', '.join([s['year'] for s in result_stats])}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun fichier DIS_RESULT √† traiter\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2827d08-f4aa-431c-9233-88c5d0dc2272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # üóëÔ∏è Suppression des tables annuelles\n",
    "# print(\"üóëÔ∏è  Suppression des tables annuelles...\\n\")\n",
    "\n",
    "# tables_to_drop = [\n",
    "#     'dis_plv_2021', 'dis_plv_2022', 'dis_plv_2023', 'dis_plv_2024', 'dis_plv_2025',\n",
    "#     'dis_result_2021', 'dis_result_2022', 'dis_result_2023', 'dis_result_2024', 'dis_result_2025'\n",
    "# ]\n",
    "\n",
    "# for table in tables_to_drop:\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP TABLE IF EXISTS eau_potable.{table}\")\n",
    "#         print(f\"   ‚úÖ Table '{table}' supprim√©e\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ùå Erreur: {e}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"üìã Tables restantes:\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# result = spark.sql(\"SHOW TABLES IN eau_potable\").collect()\n",
    "# print(f\"\\n   Total: {len(result)} tables\\n\")\n",
    "# for row in result:\n",
    "#     print(f\"   ‚úÖ {row.tableName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccdef09-8c3a-4e05-9117-7babfa490538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå CELLULE 6 : V√©rification des tables cr√©√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4aab4d-6d63-4aee-b8d7-5ea8416b86fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç V√âRIFICATION DES TABLES CR√â√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Lister toutes les tables de la base\n",
    "tables = spark.sql(f\"SHOW TABLES IN {DATABASE_NAME}\").collect()\n",
    "\n",
    "print(f\"\\nüìã Tables dans '{DATABASE_NAME}' :\")\n",
    "print(f\"   Total : {len(tables)} tables\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"   ‚úÖ {table.tableName}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56bfd4e8-d5cd-437a-b2f2-e100bf1d3ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå CELLULE 7 : Statistiques d√©taill√©es des tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42ed2aa-5e43-46d3-8718-7b68c745d958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä STATISTIQUES DES TABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Statistiques DIS_PLV\n",
    "print(\"\\nüîµ Table DIS_PLV :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_plv = spark.table(f\"{DATABASE_NAME}.dis_plv\")\n",
    "    total_plv = df_plv.count()\n",
    "    nb_colonnes_plv = len(df_plv.columns)\n",
    "    \n",
    "    print(f\"   üìä Lignes totales : {total_plv:,}\")\n",
    "    print(f\"   üìä Colonnes : {nb_colonnes_plv}\")\n",
    "    \n",
    "    # Compter par ann√©e\n",
    "    print(f\"\\n   üìÖ R√©partition par ann√©e :\")\n",
    "    df_plv.groupBy(\"annee\").count().orderBy(\"annee\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Statistiques DIS_RESULT\n",
    "print(\"\\nüü¢ Table DIS_RESULT :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_result = spark.table(f\"{DATABASE_NAME}.dis_result\")\n",
    "    total_result = df_result.count()\n",
    "    nb_colonnes_result = len(df_result.columns)\n",
    "    \n",
    "    print(f\"   üìä Lignes totales : {total_result:,}\")\n",
    "    print(f\"   üìä Colonnes : {nb_colonnes_result}\")\n",
    "    \n",
    "    # Compter par ann√©e\n",
    "    print(f\"\\n   üìÖ R√©partition par ann√©e :\")\n",
    "    df_result.groupBy(\"annee\").count().orderBy(\"annee\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b151f14f-f5ee-4c7e-8eca-2a6fa1357726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå CELLULE 8 : Sch√©ma des tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb94403-850e-4823-af7d-a0e185bd5afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SCH√âMA DES TABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sch√©ma DIS_PLV\n",
    "print(f\"\\nüîµ Sch√©ma de 'dis_plv' :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_plv = spark.table(f\"{DATABASE_NAME}.dis_plv\")\n",
    "    df_plv.printSchema()\n",
    "    \n",
    "    print(f\"\\nüìã Colonnes ({len(df_plv.columns)}) :\")\n",
    "    for idx, col_name in enumerate(df_plv.columns, 1):\n",
    "        print(f\"   {idx:2d}. {col_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Sch√©ma DIS_RESULT\n",
    "print(f\"\\nüü¢ Sch√©ma de 'dis_result' :\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    df_result = spark.table(f\"{DATABASE_NAME}.dis_result\")\n",
    "    df_result.printSchema()\n",
    "    \n",
    "    print(f\"\\nüìã Colonnes ({len(df_result.columns)}) :\")\n",
    "    for idx, col_name in enumerate(df_result.columns, 1):\n",
    "        print(f\"   {idx:2d}. {col_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6124cd-f153-4328-b3f1-ac9ed492c400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå CELLULE 9 : Aper√ßu des donn√©es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065bab00-ef39-44fa-aba9-df69e835ec4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üëÄ APER√áU DES DONN√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Aper√ßu DIS_PLV\n",
    "print(f\"\\nüìÑ Aper√ßu de 'dis_plv' (5 premi√®res lignes) :\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"SELECT * FROM {DATABASE_NAME}.dis_plv LIMIT 5\").show(truncate=50, vertical=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Aper√ßu DIS_RESULT\n",
    "print(f\"\\nüìÑ Aper√ßu de 'dis_result' (5 premi√®res lignes) :\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"SELECT * FROM {DATABASE_NAME}.dis_result LIMIT 5\").show(truncate=50, vertical=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6496f07a-86bc-485b-b8ed-ba8000536449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå CELLULE 10 : Exemples de requ√™tes SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a4a246-62ef-4a34-9d26-d0f7e82230c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° EXEMPLES DE REQU√äTES SQL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Exemple 1 : Donn√©es PLV pour un pr√©l√®vement sp√©cifique\n",
    "print(f\"\\nüîµ Exemple 1 : Donn√©es PLV pour referenceprel = 00100143925\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            referenceprel,\n",
    "            cddept,\n",
    "            nomcommuneprinc,\n",
    "            dateprel,\n",
    "            heureprel,\n",
    "            conclusionprel,\n",
    "            plvconformitebacterio,\n",
    "            plvconformitechimique,\n",
    "            ugelib,\n",
    "            distrlib\n",
    "        FROM {DATABASE_NAME}.dis_plv\n",
    "        WHERE referenceprel = '00100143925'\n",
    "    \"\"\").show(truncate=False, vertical=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 2 : R√©sultats d'analyses pour ce m√™me pr√©l√®vement\n",
    "print(f\"\\nüü¢ Exemple 2 : R√©sultats d'analyses pour referenceprel = 00100143925\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            referenceprel,\n",
    "            cdparametre,\n",
    "            libmajparametre,\n",
    "            valtraduite,\n",
    "            cdunitereference,\n",
    "            limitequal,\n",
    "            refqual\n",
    "        FROM {DATABASE_NAME}.dis_result\n",
    "        WHERE referenceprel = '00100143925'\n",
    "        ORDER BY libmajparametre\n",
    "    \"\"\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 3 : Jointure compl√®te PLV + RESULT pour ce pr√©l√®vement\n",
    "print(f\"\\nüîó Exemple 3 : Jointure PLV + RESULT pour referenceprel = 00100143925\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            p.referenceprel,\n",
    "            p.nomcommuneprinc,\n",
    "            p.dateprel,\n",
    "            p.heureprel,\n",
    "            p.plvconformitebacterio,\n",
    "            p.plvconformitechimique,\n",
    "            r.libmajparametre,\n",
    "            r.valtraduite,\n",
    "            r.cdunitereference,\n",
    "            r.limitequal,\n",
    "            r.refqual\n",
    "        FROM {DATABASE_NAME}.dis_plv p\n",
    "        INNER JOIN {DATABASE_NAME}.dis_result r\n",
    "            ON p.referenceprel = r.referenceprel\n",
    "            AND p.cddept = r.cddept\n",
    "        WHERE p.referenceprel = '00100143925'\n",
    "        ORDER BY r.libmajparametre\n",
    "    \"\"\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 4 : Nombre de param√®tres analys√©s pour ce pr√©l√®vement\n",
    "print(f\"\\nüìä Exemple 4 : Statistiques pour referenceprel = 00100143925\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            p.referenceprel,\n",
    "            p.nomcommuneprinc,\n",
    "            p.dateprel,\n",
    "            COUNT(r.cdparametre) as nb_parametres_analyses,\n",
    "            p.plvconformitebacterio,\n",
    "            p.plvconformitechimique\n",
    "        FROM {DATABASE_NAME}.dis_plv p\n",
    "        LEFT JOIN {DATABASE_NAME}.dis_result r\n",
    "            ON p.referenceprel = r.referenceprel\n",
    "            AND p.cddept = r.cddept\n",
    "        WHERE p.referenceprel = '00100143925'\n",
    "        GROUP BY \n",
    "            p.referenceprel,\n",
    "            p.nomcommuneprinc,\n",
    "            p.dateprel,\n",
    "            p.plvconformitebacterio,\n",
    "            p.plvconformitechimique\n",
    "    \"\"\").show(truncate=False, vertical=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 5 : Top 10 d√©partements par nombre de pr√©l√®vements\n",
    "print(f\"\\nüîµ Exemple 5 : Nombre de pr√©l√®vements par d√©partement (TOP 10)\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            cddept,\n",
    "            COUNT(*) as nb_prelevements\n",
    "        FROM {DATABASE_NAME}.dis_plv\n",
    "        GROUP BY cddept\n",
    "        ORDER BY nb_prelevements DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 6 : Top 10 param√®tres les plus analys√©s\n",
    "print(f\"\\nüü¢ Exemple 6 : Top 10 param√®tres les plus analys√©s\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            libmajparametre,\n",
    "            COUNT(*) as nb_analyses\n",
    "        FROM {DATABASE_NAME}.dis_result\n",
    "        GROUP BY libmajparametre\n",
    "        ORDER BY nb_analyses DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=50)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "# Exemple 7 : Jointure g√©n√©rale PLV + RESULT (√©chantillon)\n",
    "print(f\"\\nüîó Exemple 7 : Jointure PLV + RESULT (√©chantillon de 10 lignes)\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            p.referenceprel,\n",
    "            p.nomcommuneprinc,\n",
    "            p.dateprel,\n",
    "            r.libmajparametre,\n",
    "            r.valtraduite,\n",
    "            r.cdunitereference\n",
    "        FROM {DATABASE_NAME}.dis_plv p\n",
    "        INNER JOIN {DATABASE_NAME}.dis_result r\n",
    "            ON p.referenceprel = r.referenceprel \n",
    "            AND p.cddept = r.cddept\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=40)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {str(e)}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Exemples de requ√™tes SQL termin√©s\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_create_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
